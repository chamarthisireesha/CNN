{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0e534f",
   "metadata": {},
   "source": [
    "ipykernel\n",
    "\n",
    "\n",
    "jupyter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638f56e",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/taraprole/roadsignclassifier\n",
    "    \n",
    "to use resnet: https://www.pluralsight.com/guides/introduction-to-resnet    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d835b7d",
   "metadata": {},
   "source": [
    "to download datasets: https://github.com/surmenok/GTSRB/blob/master/german-traffic-signs.ipynb\n",
    "\n",
    "https://datasets.activeloop.ai/docs/ml/datasets/gtsrb-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b727c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4faa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms,models\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc2d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d9fd0",
   "metadata": {},
   "source": [
    "transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a457430",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b347b8",
   "metadata": {},
   "source": [
    "hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34bad7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "learning_rate = 0.001\n",
    "EPOCHS = 7\n",
    "numClasses = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c80565",
   "metadata": {},
   "source": [
    "to read from train csv : https://www.kaggle.com/code/harshit1900/german-traffic-signs-beginners/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cd3674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/gtsrb-train loaded successfully.\n",
      "\n",
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/gtsrb-train\n",
      "\n",
      " \r"
     ]
    }
   ],
   "source": [
    "ds_train = deeplake.load(\"hub://activeloop/gtsrb-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef0f3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'boxes', 'labels', 'shapes', 'colors'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.tensors.keys()    # dict_keys(['images', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6916ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18], dtype=uint32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.labels[0].numpy() # array([6], dtype=uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b71371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/gtsrb-test loaded successfully.\n",
      "\n",
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/gtsrb-test\n",
      "\n",
      " \r"
     ]
    }
   ],
   "source": [
    "ds_test = deeplake.load(\"hub://activeloop/gtsrb-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4974510c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'boxes', 'labels', 'shapes', 'colors'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.tensors.keys()    # dict_keys(['images', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7ca6dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11], dtype=uint32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.labels[0].numpy() # array([6], dtype=uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1218e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cham_si/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/deeplake/integrations/pytorch/common.py:91: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader = ds_train.pytorch(num_workers=0, batch_size=4, transform={\n",
    "                        'images': data_transforms, 'labels': None}, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a4c3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pedestrian',\n",
       " 'speed_limit_20km/hr',\n",
       " 'speed_limit_30km⁄hr',\n",
       " 'no_passing_of _vehicle_over_3.5ton',\n",
       " 'crossroads',\n",
       " 'priority_road_sign',\n",
       " 'yield',\n",
       " 'stop',\n",
       " 'no_vehicles',\n",
       " 'vehicle_over_3.5ton_prohibited',\n",
       " 'no_entry',\n",
       " 'general_caution',\n",
       " 'dangerous_curve_to_left',\n",
       " 'speed_limit_50km⁄hr',\n",
       " 'dangerous_curve_to_right',\n",
       " 'double_curve',\n",
       " 'bumpy_road',\n",
       " 'slippery_road',\n",
       " 'narrow_road_on_right',\n",
       " 'road_work',\n",
       " 'traffic_sign',\n",
       " 'chikdren_crossing',\n",
       " 'bicycle_crossing',\n",
       " 'speed_limit_60km⁄hr',\n",
       " 'beware_of_ice/snow',\n",
       " 'wild_animal_crossing',\n",
       " 'end_of_all_speed_and_passing_limit',\n",
       " 'compulsory_turn_right',\n",
       " 'compulsory_turn_left',\n",
       " 'straight_ahead',\n",
       " 'go_straight_or_right',\n",
       " 'go_straight_or_left',\n",
       " 'avoid_obstacle_on_right',\n",
       " 'avoid_obstacle_on_left',\n",
       " 'speed_limit_70km⁄hr',\n",
       " 'roundabout',\n",
       " 'end_of_no_passing',\n",
       " 'end_of_no_passing_by_vehicle_over_3.5ton',\n",
       " 'speed_limit_80km⁄hr',\n",
       " 'end_of_speed_limit_80km/hr',\n",
       " 'speed_limit_100km⁄hr',\n",
       " 'speed_limit_120km⁄hr',\n",
       " 'no_passing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.labels.info.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "635c0dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train.labels.info.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a48ed410",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = ds_test.pytorch(num_workers=0, batch_size=4, transform={\n",
    "                        'images': data_transforms, 'labels': None}, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab59d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cham_si/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cham_si/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=43, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = models.resnet50(pretrained=True)\n",
    "net.fc = torch.nn.Linear(net.fc.in_features, len(ds_train.labels.info.class_names))\n",
    "net.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20df8d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89a8e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda() if device else net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12129657",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad468b48",
   "metadata": {},
   "source": [
    "training for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1032e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:010, Epoch:001, Training: Loss: 0.8379 , Accuracy: 0.7500\n",
      "\"Batch number:020, Epoch:001, Training: Loss: 1.8231 , Accuracy: 0.7500\n",
      "\"Batch number:030, Epoch:001, Training: Loss: 1.5980 , Accuracy: 0.7500\n",
      "\"Batch number:040, Epoch:001, Training: Loss: 0.9463 , Accuracy: 0.7500\n",
      "\"Batch number:050, Epoch:001, Training: Loss: 0.6136 , Accuracy: 0.5000\n",
      "\"Batch number:060, Epoch:001, Training: Loss: 1.0740 , Accuracy: 0.5000\n",
      "\"Batch number:070, Epoch:001, Training: Loss: 0.5134 , Accuracy: 0.7500\n",
      "\"Batch number:080, Epoch:001, Training: Loss: 0.1675 , Accuracy: 1.0000\n",
      "\"Batch number:090, Epoch:001, Training: Loss: 0.9848 , Accuracy: 0.5000\n",
      "\"Batch number:100, Epoch:001, Training: Loss: 0.1927 , Accuracy: 1.0000\n",
      "\"Batch number:110, Epoch:001, Training: Loss: 3.1571 , Accuracy: 0.2500\n",
      "\"Batch number:120, Epoch:001, Training: Loss: 1.0098 , Accuracy: 0.5000\n",
      "\"Batch number:130, Epoch:001, Training: Loss: 0.3702 , Accuracy: 1.0000\n",
      "\"Batch number:140, Epoch:001, Training: Loss: 0.0363 , Accuracy: 1.0000\n",
      "\"Batch number:150, Epoch:001, Training: Loss: 0.0362 , Accuracy: 1.0000\n",
      "\"Batch number:160, Epoch:001, Training: Loss: 0.0172 , Accuracy: 1.0000\n",
      "\"Batch number:170, Epoch:001, Training: Loss: 0.1361 , Accuracy: 1.0000\n",
      "\"Batch number:180, Epoch:001, Training: Loss: 1.0006 , Accuracy: 0.5000\n",
      "\"Batch number:190, Epoch:001, Training: Loss: 0.9479 , Accuracy: 0.5000\n",
      "\"Batch number:200, Epoch:001, Training: Loss: 0.6566 , Accuracy: 0.5000\n",
      "\"Batch number:210, Epoch:001, Training: Loss: 0.0870 , Accuracy: 1.0000\n",
      "\"Batch number:220, Epoch:001, Training: Loss: 0.0871 , Accuracy: 1.0000\n",
      "\"Batch number:230, Epoch:001, Training: Loss: 0.0333 , Accuracy: 1.0000\n",
      "\"Batch number:240, Epoch:001, Training: Loss: 0.1241 , Accuracy: 1.0000\n",
      "\"Batch number:250, Epoch:001, Training: Loss: 0.0186 , Accuracy: 1.0000\n",
      "\"Batch number:260, Epoch:001, Training: Loss: 0.3647 , Accuracy: 0.7500\n",
      "\"Batch number:270, Epoch:001, Training: Loss: 0.1456 , Accuracy: 1.0000\n",
      "\"Batch number:280, Epoch:001, Training: Loss: 1.5245 , Accuracy: 0.5000\n",
      "\"Batch number:290, Epoch:001, Training: Loss: 0.0275 , Accuracy: 1.0000\n",
      "\"Batch number:300, Epoch:001, Training: Loss: 0.0475 , Accuracy: 1.0000\n",
      "\"Batch number:310, Epoch:001, Training: Loss: 0.0256 , Accuracy: 1.0000\n",
      "\"Batch number:320, Epoch:001, Training: Loss: 0.0091 , Accuracy: 1.0000\n",
      "\"Batch number:330, Epoch:001, Training: Loss: 0.0958 , Accuracy: 1.0000\n",
      "\"Batch number:340, Epoch:001, Training: Loss: 0.0118 , Accuracy: 1.0000\n",
      "\"Batch number:350, Epoch:001, Training: Loss: 0.0052 , Accuracy: 1.0000\n",
      "\"Batch number:360, Epoch:001, Training: Loss: 0.0241 , Accuracy: 1.0000\n",
      "\"Batch number:370, Epoch:001, Training: Loss: 0.0683 , Accuracy: 1.0000\n",
      "\"Batch number:380, Epoch:001, Training: Loss: 0.0118 , Accuracy: 1.0000\n",
      "\"Batch number:390, Epoch:001, Training: Loss: 0.0110 , Accuracy: 1.0000\n",
      "\"Batch number:400, Epoch:001, Training: Loss: 0.0113 , Accuracy: 1.0000\n",
      "\"Batch number:410, Epoch:001, Training: Loss: 0.0113 , Accuracy: 1.0000\n",
      "\"Batch number:420, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n",
      "\"Batch number:430, Epoch:001, Training: Loss: 0.0021 , Accuracy: 1.0000\n",
      "\"Batch number:440, Epoch:001, Training: Loss: 0.0037 , Accuracy: 1.0000\n",
      "\"Batch number:450, Epoch:001, Training: Loss: 0.0033 , Accuracy: 1.0000\n",
      "\"Batch number:460, Epoch:001, Training: Loss: 0.7062 , Accuracy: 0.7500\n",
      "\"Batch number:470, Epoch:001, Training: Loss: 0.0480 , Accuracy: 1.0000\n",
      "\"Batch number:480, Epoch:001, Training: Loss: 0.0029 , Accuracy: 1.0000\n",
      "\"Batch number:490, Epoch:001, Training: Loss: 0.0007 , Accuracy: 1.0000\n",
      "\"Batch number:500, Epoch:001, Training: Loss: 0.0767 , Accuracy: 1.0000\n",
      "\"Batch number:510, Epoch:001, Training: Loss: 0.0108 , Accuracy: 1.0000\n",
      "\"Batch number:520, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:530, Epoch:001, Training: Loss: 0.0006 , Accuracy: 1.0000\n",
      "\"Batch number:540, Epoch:001, Training: Loss: 0.1657 , Accuracy: 1.0000\n",
      "\"Batch number:550, Epoch:001, Training: Loss: 0.1662 , Accuracy: 1.0000\n",
      "\"Batch number:560, Epoch:001, Training: Loss: 0.0014 , Accuracy: 1.0000\n",
      "\"Batch number:570, Epoch:001, Training: Loss: 0.0006 , Accuracy: 1.0000\n",
      "\"Batch number:580, Epoch:001, Training: Loss: 0.1300 , Accuracy: 1.0000\n",
      "\"Batch number:590, Epoch:001, Training: Loss: 0.0035 , Accuracy: 1.0000\n",
      "\"Batch number:600, Epoch:001, Training: Loss: 0.0206 , Accuracy: 1.0000\n",
      "\"Batch number:610, Epoch:001, Training: Loss: 0.0004 , Accuracy: 1.0000\n",
      "\"Batch number:620, Epoch:001, Training: Loss: 0.0038 , Accuracy: 1.0000\n",
      "\"Batch number:630, Epoch:001, Training: Loss: 0.0217 , Accuracy: 1.0000\n",
      "\"Batch number:640, Epoch:001, Training: Loss: 0.0171 , Accuracy: 1.0000\n",
      "\"Batch number:650, Epoch:001, Training: Loss: 0.0028 , Accuracy: 1.0000\n",
      "\"Batch number:660, Epoch:001, Training: Loss: 0.0011 , Accuracy: 1.0000\n",
      "\"Batch number:670, Epoch:001, Training: Loss: 0.0337 , Accuracy: 1.0000\n",
      "\"Batch number:680, Epoch:001, Training: Loss: 0.0137 , Accuracy: 1.0000\n",
      "\"Batch number:690, Epoch:001, Training: Loss: 0.0704 , Accuracy: 1.0000\n",
      "\"Batch number:700, Epoch:001, Training: Loss: 0.0102 , Accuracy: 1.0000\n",
      "\"Batch number:710, Epoch:001, Training: Loss: 0.0079 , Accuracy: 1.0000\n",
      "\"Batch number:720, Epoch:001, Training: Loss: 0.0025 , Accuracy: 1.0000\n",
      "\"Batch number:730, Epoch:001, Training: Loss: 0.0321 , Accuracy: 1.0000\n",
      "\"Batch number:740, Epoch:001, Training: Loss: 0.0009 , Accuracy: 1.0000\n",
      "\"Batch number:750, Epoch:001, Training: Loss: 0.0221 , Accuracy: 1.0000\n",
      "\"Batch number:760, Epoch:001, Training: Loss: 0.3907 , Accuracy: 0.7500\n",
      "\"Batch number:770, Epoch:001, Training: Loss: 0.0021 , Accuracy: 1.0000\n",
      "\"Batch number:780, Epoch:001, Training: Loss: 0.0034 , Accuracy: 1.0000\n",
      "\"Batch number:790, Epoch:001, Training: Loss: 0.0016 , Accuracy: 1.0000\n",
      "\"Batch number:800, Epoch:001, Training: Loss: 0.0006 , Accuracy: 1.0000\n",
      "\"Batch number:810, Epoch:001, Training: Loss: 0.0045 , Accuracy: 1.0000\n",
      "\"Batch number:820, Epoch:001, Training: Loss: 1.8419 , Accuracy: 0.7500\n",
      "\"Batch number:830, Epoch:001, Training: Loss: 0.0130 , Accuracy: 1.0000\n",
      "\"Batch number:840, Epoch:001, Training: Loss: 0.0227 , Accuracy: 1.0000\n",
      "\"Batch number:850, Epoch:001, Training: Loss: 0.0078 , Accuracy: 1.0000\n",
      "\"Batch number:860, Epoch:001, Training: Loss: 8.7988 , Accuracy: 0.2500\n",
      "\"Batch number:870, Epoch:001, Training: Loss: 3.2614 , Accuracy: 0.2500\n",
      "\"Batch number:880, Epoch:001, Training: Loss: 3.0521 , Accuracy: 0.0000\n",
      "\"Batch number:890, Epoch:001, Training: Loss: 1.4004 , Accuracy: 0.5000\n",
      "\"Batch number:900, Epoch:001, Training: Loss: 1.9797 , Accuracy: 0.2500\n",
      "\"Batch number:910, Epoch:001, Training: Loss: 1.8512 , Accuracy: 0.2500\n",
      "\"Batch number:920, Epoch:001, Training: Loss: 1.4518 , Accuracy: 0.5000\n",
      "\"Batch number:930, Epoch:001, Training: Loss: 1.5938 , Accuracy: 0.7500\n",
      "\"Batch number:940, Epoch:001, Training: Loss: 0.7373 , Accuracy: 1.0000\n",
      "\"Batch number:950, Epoch:001, Training: Loss: 2.6572 , Accuracy: 0.2500\n",
      "\"Batch number:960, Epoch:001, Training: Loss: 2.2520 , Accuracy: 0.7500\n",
      "\"Batch number:970, Epoch:001, Training: Loss: 0.9540 , Accuracy: 0.7500\n",
      "\"Batch number:980, Epoch:001, Training: Loss: 1.2834 , Accuracy: 0.2500\n",
      "\"Batch number:990, Epoch:001, Training: Loss: 0.7295 , Accuracy: 0.7500\n",
      "\"Batch number:1000, Epoch:001, Training: Loss: 0.7448 , Accuracy: 0.7500\n",
      "\"Batch number:1010, Epoch:001, Training: Loss: 1.3507 , Accuracy: 0.5000\n",
      "\"Batch number:1020, Epoch:001, Training: Loss: 0.4147 , Accuracy: 1.0000\n",
      "\"Batch number:1030, Epoch:001, Training: Loss: 1.0828 , Accuracy: 0.7500\n",
      "\"Batch number:1040, Epoch:001, Training: Loss: 0.1063 , Accuracy: 1.0000\n",
      "\"Batch number:1050, Epoch:001, Training: Loss: 1.4640 , Accuracy: 0.5000\n",
      "\"Batch number:1060, Epoch:001, Training: Loss: 1.4011 , Accuracy: 0.5000\n",
      "\"Batch number:1070, Epoch:001, Training: Loss: 0.7876 , Accuracy: 0.7500\n",
      "\"Batch number:1080, Epoch:001, Training: Loss: 0.9986 , Accuracy: 0.7500\n",
      "\"Batch number:1090, Epoch:001, Training: Loss: 0.0880 , Accuracy: 1.0000\n",
      "\"Batch number:1100, Epoch:001, Training: Loss: 0.0785 , Accuracy: 1.0000\n",
      "\"Batch number:1110, Epoch:001, Training: Loss: 0.6434 , Accuracy: 0.5000\n",
      "\"Batch number:1120, Epoch:001, Training: Loss: 0.5391 , Accuracy: 0.7500\n",
      "\"Batch number:1130, Epoch:001, Training: Loss: 0.5614 , Accuracy: 1.0000\n",
      "\"Batch number:1140, Epoch:001, Training: Loss: 1.9349 , Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:1150, Epoch:001, Training: Loss: 0.8605 , Accuracy: 0.7500\n",
      "\"Batch number:1160, Epoch:001, Training: Loss: 0.3015 , Accuracy: 1.0000\n",
      "\"Batch number:1170, Epoch:001, Training: Loss: 2.6941 , Accuracy: 0.0000\n",
      "\"Batch number:1180, Epoch:001, Training: Loss: 0.1159 , Accuracy: 1.0000\n",
      "\"Batch number:1190, Epoch:001, Training: Loss: 0.3481 , Accuracy: 0.7500\n",
      "\"Batch number:1200, Epoch:001, Training: Loss: 0.4950 , Accuracy: 1.0000\n",
      "\"Batch number:1210, Epoch:001, Training: Loss: 1.6396 , Accuracy: 0.7500\n",
      "\"Batch number:1220, Epoch:001, Training: Loss: 0.6859 , Accuracy: 1.0000\n",
      "\"Batch number:1230, Epoch:001, Training: Loss: 0.0270 , Accuracy: 1.0000\n",
      "\"Batch number:1240, Epoch:001, Training: Loss: 0.3719 , Accuracy: 1.0000\n",
      "\"Batch number:1250, Epoch:001, Training: Loss: 0.0472 , Accuracy: 1.0000\n",
      "\"Batch number:1260, Epoch:001, Training: Loss: 0.3341 , Accuracy: 1.0000\n",
      "\"Batch number:1270, Epoch:001, Training: Loss: 0.4582 , Accuracy: 0.7500\n",
      "\"Batch number:1280, Epoch:001, Training: Loss: 0.0721 , Accuracy: 1.0000\n",
      "\"Batch number:1290, Epoch:001, Training: Loss: 0.4527 , Accuracy: 0.7500\n",
      "\"Batch number:1300, Epoch:001, Training: Loss: 0.5763 , Accuracy: 0.5000\n",
      "\"Batch number:1310, Epoch:001, Training: Loss: 0.0814 , Accuracy: 1.0000\n",
      "\"Batch number:1320, Epoch:001, Training: Loss: 0.0682 , Accuracy: 1.0000\n",
      "\"Batch number:1330, Epoch:001, Training: Loss: 0.0746 , Accuracy: 1.0000\n",
      "\"Batch number:1340, Epoch:001, Training: Loss: 0.0572 , Accuracy: 1.0000\n",
      "\"Batch number:1350, Epoch:001, Training: Loss: 0.0417 , Accuracy: 1.0000\n",
      "\"Batch number:1360, Epoch:001, Training: Loss: 0.1410 , Accuracy: 1.0000\n",
      "\"Batch number:1370, Epoch:001, Training: Loss: 0.0982 , Accuracy: 1.0000\n",
      "\"Batch number:1380, Epoch:001, Training: Loss: 0.8148 , Accuracy: 0.5000\n",
      "\"Batch number:1390, Epoch:001, Training: Loss: 0.8598 , Accuracy: 0.7500\n",
      "\"Batch number:1400, Epoch:001, Training: Loss: 0.4205 , Accuracy: 0.7500\n",
      "\"Batch number:1410, Epoch:001, Training: Loss: 0.1717 , Accuracy: 1.0000\n",
      "\"Batch number:1420, Epoch:001, Training: Loss: 0.0116 , Accuracy: 1.0000\n",
      "\"Batch number:1430, Epoch:001, Training: Loss: 0.2931 , Accuracy: 1.0000\n",
      "\"Batch number:1440, Epoch:001, Training: Loss: 0.5271 , Accuracy: 0.7500\n",
      "\"Batch number:1450, Epoch:001, Training: Loss: 0.0978 , Accuracy: 1.0000\n",
      "\"Batch number:1460, Epoch:001, Training: Loss: 0.1188 , Accuracy: 1.0000\n",
      "\"Batch number:1470, Epoch:001, Training: Loss: 0.0266 , Accuracy: 1.0000\n",
      "\"Batch number:1480, Epoch:001, Training: Loss: 0.0806 , Accuracy: 1.0000\n",
      "\"Batch number:1490, Epoch:001, Training: Loss: 0.0195 , Accuracy: 1.0000\n",
      "\"Batch number:1500, Epoch:001, Training: Loss: 0.3055 , Accuracy: 1.0000\n",
      "\"Batch number:1510, Epoch:001, Training: Loss: 0.0267 , Accuracy: 1.0000\n",
      "\"Batch number:1520, Epoch:001, Training: Loss: 0.0107 , Accuracy: 1.0000\n",
      "\"Batch number:1530, Epoch:001, Training: Loss: 0.0127 , Accuracy: 1.0000\n",
      "\"Batch number:1540, Epoch:001, Training: Loss: 0.5475 , Accuracy: 0.7500\n",
      "\"Batch number:1550, Epoch:001, Training: Loss: 0.3577 , Accuracy: 0.7500\n",
      "\"Batch number:1560, Epoch:001, Training: Loss: 0.0189 , Accuracy: 1.0000\n",
      "\"Batch number:1570, Epoch:001, Training: Loss: 0.8863 , Accuracy: 0.7500\n",
      "\"Batch number:1580, Epoch:001, Training: Loss: 0.2542 , Accuracy: 0.7500\n",
      "\"Batch number:1590, Epoch:001, Training: Loss: 0.0209 , Accuracy: 1.0000\n",
      "\"Batch number:1600, Epoch:001, Training: Loss: 0.0491 , Accuracy: 1.0000\n",
      "\"Batch number:1610, Epoch:001, Training: Loss: 0.0739 , Accuracy: 1.0000\n",
      "\"Batch number:1620, Epoch:001, Training: Loss: 0.1204 , Accuracy: 1.0000\n",
      "\"Batch number:1630, Epoch:001, Training: Loss: 0.0126 , Accuracy: 1.0000\n",
      "\"Batch number:1640, Epoch:001, Training: Loss: 0.0761 , Accuracy: 1.0000\n",
      "\"Batch number:1650, Epoch:001, Training: Loss: 1.4552 , Accuracy: 0.7500\n",
      "\"Batch number:1660, Epoch:001, Training: Loss: 0.0267 , Accuracy: 1.0000\n",
      "\"Batch number:1670, Epoch:001, Training: Loss: 0.0465 , Accuracy: 1.0000\n",
      "\"Batch number:1680, Epoch:001, Training: Loss: 0.2744 , Accuracy: 0.7500\n",
      "\"Batch number:1690, Epoch:001, Training: Loss: 0.0335 , Accuracy: 1.0000\n",
      "\"Batch number:1700, Epoch:001, Training: Loss: 0.0459 , Accuracy: 1.0000\n",
      "\"Batch number:1710, Epoch:001, Training: Loss: 0.0784 , Accuracy: 1.0000\n",
      "\"Batch number:1720, Epoch:001, Training: Loss: 0.3832 , Accuracy: 1.0000\n",
      "\"Batch number:1730, Epoch:001, Training: Loss: 0.0020 , Accuracy: 1.0000\n",
      "\"Batch number:1740, Epoch:001, Training: Loss: 0.0088 , Accuracy: 1.0000\n",
      "\"Batch number:1750, Epoch:001, Training: Loss: 0.8611 , Accuracy: 0.7500\n",
      "\"Batch number:1760, Epoch:001, Training: Loss: 0.0143 , Accuracy: 1.0000\n",
      "\"Batch number:1770, Epoch:001, Training: Loss: 0.4533 , Accuracy: 0.7500\n",
      "\"Batch number:1780, Epoch:001, Training: Loss: 0.0847 , Accuracy: 1.0000\n",
      "\"Batch number:1790, Epoch:001, Training: Loss: 0.8943 , Accuracy: 0.5000\n",
      "\"Batch number:1800, Epoch:001, Training: Loss: 0.0075 , Accuracy: 1.0000\n",
      "\"Batch number:1810, Epoch:001, Training: Loss: 0.2071 , Accuracy: 1.0000\n",
      "\"Batch number:1820, Epoch:001, Training: Loss: 0.1240 , Accuracy: 1.0000\n",
      "\"Batch number:1830, Epoch:001, Training: Loss: 0.0425 , Accuracy: 1.0000\n",
      "\"Batch number:1840, Epoch:001, Training: Loss: 0.0166 , Accuracy: 1.0000\n",
      "\"Batch number:1850, Epoch:001, Training: Loss: 0.0063 , Accuracy: 1.0000\n",
      "\"Batch number:1860, Epoch:001, Training: Loss: 0.0057 , Accuracy: 1.0000\n",
      "\"Batch number:1870, Epoch:001, Training: Loss: 0.0104 , Accuracy: 1.0000\n",
      "\"Batch number:1880, Epoch:001, Training: Loss: 0.0058 , Accuracy: 1.0000\n",
      "\"Batch number:1890, Epoch:001, Training: Loss: 0.0503 , Accuracy: 1.0000\n",
      "\"Batch number:1900, Epoch:001, Training: Loss: 0.0235 , Accuracy: 1.0000\n",
      "\"Batch number:1910, Epoch:001, Training: Loss: 0.0111 , Accuracy: 1.0000\n",
      "\"Batch number:1920, Epoch:001, Training: Loss: 0.0153 , Accuracy: 1.0000\n",
      "\"Batch number:1930, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:1940, Epoch:001, Training: Loss: 0.0200 , Accuracy: 1.0000\n",
      "\"Batch number:1950, Epoch:001, Training: Loss: 0.0226 , Accuracy: 1.0000\n",
      "\"Batch number:1960, Epoch:001, Training: Loss: 0.0164 , Accuracy: 1.0000\n",
      "\"Batch number:1970, Epoch:001, Training: Loss: 0.4076 , Accuracy: 0.7500\n",
      "\"Batch number:1980, Epoch:001, Training: Loss: 0.0262 , Accuracy: 1.0000\n",
      "\"Batch number:1990, Epoch:001, Training: Loss: 0.0838 , Accuracy: 1.0000\n",
      "\"Batch number:2000, Epoch:001, Training: Loss: 0.2731 , Accuracy: 1.0000\n",
      "\"Batch number:2010, Epoch:001, Training: Loss: 0.2816 , Accuracy: 1.0000\n",
      "\"Batch number:2020, Epoch:001, Training: Loss: 0.0942 , Accuracy: 1.0000\n",
      "\"Batch number:2030, Epoch:001, Training: Loss: 0.0072 , Accuracy: 1.0000\n",
      "\"Batch number:2040, Epoch:001, Training: Loss: 0.0245 , Accuracy: 1.0000\n",
      "\"Batch number:2050, Epoch:001, Training: Loss: 0.2318 , Accuracy: 1.0000\n",
      "\"Batch number:2060, Epoch:001, Training: Loss: 0.0043 , Accuracy: 1.0000\n",
      "\"Batch number:2070, Epoch:001, Training: Loss: 0.0415 , Accuracy: 1.0000\n",
      "\"Batch number:2080, Epoch:001, Training: Loss: 0.0657 , Accuracy: 1.0000\n",
      "\"Batch number:2090, Epoch:001, Training: Loss: 0.0125 , Accuracy: 1.0000\n",
      "\"Batch number:2100, Epoch:001, Training: Loss: 0.0041 , Accuracy: 1.0000\n",
      "\"Batch number:2110, Epoch:001, Training: Loss: 0.0221 , Accuracy: 1.0000\n",
      "\"Batch number:2120, Epoch:001, Training: Loss: 0.0009 , Accuracy: 1.0000\n",
      "\"Batch number:2130, Epoch:001, Training: Loss: 0.0021 , Accuracy: 1.0000\n",
      "\"Batch number:2140, Epoch:001, Training: Loss: 0.0203 , Accuracy: 1.0000\n",
      "\"Batch number:2150, Epoch:001, Training: Loss: 0.0186 , Accuracy: 1.0000\n",
      "\"Batch number:2160, Epoch:001, Training: Loss: 0.0177 , Accuracy: 1.0000\n",
      "\"Batch number:2170, Epoch:001, Training: Loss: 0.0306 , Accuracy: 1.0000\n",
      "\"Batch number:2180, Epoch:001, Training: Loss: 0.0151 , Accuracy: 1.0000\n",
      "\"Batch number:2190, Epoch:001, Training: Loss: 0.0289 , Accuracy: 1.0000\n",
      "\"Batch number:2200, Epoch:001, Training: Loss: 0.0028 , Accuracy: 1.0000\n",
      "\"Batch number:2210, Epoch:001, Training: Loss: 0.0377 , Accuracy: 1.0000\n",
      "\"Batch number:2220, Epoch:001, Training: Loss: 0.0133 , Accuracy: 1.0000\n",
      "\"Batch number:2230, Epoch:001, Training: Loss: 0.0175 , Accuracy: 1.0000\n",
      "\"Batch number:2240, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:2250, Epoch:001, Training: Loss: 0.0118 , Accuracy: 1.0000\n",
      "\"Batch number:2260, Epoch:001, Training: Loss: 0.0208 , Accuracy: 1.0000\n",
      "\"Batch number:2270, Epoch:001, Training: Loss: 0.0032 , Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:2280, Epoch:001, Training: Loss: 0.0016 , Accuracy: 1.0000\n",
      "\"Batch number:2290, Epoch:001, Training: Loss: 0.0259 , Accuracy: 1.0000\n",
      "\"Batch number:2300, Epoch:001, Training: Loss: 0.0033 , Accuracy: 1.0000\n",
      "\"Batch number:2310, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:2320, Epoch:001, Training: Loss: 0.0135 , Accuracy: 1.0000\n",
      "\"Batch number:2330, Epoch:001, Training: Loss: 0.0005 , Accuracy: 1.0000\n",
      "\"Batch number:2340, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n",
      "\"Batch number:2350, Epoch:001, Training: Loss: 0.0059 , Accuracy: 1.0000\n",
      "\"Batch number:2360, Epoch:001, Training: Loss: 0.0030 , Accuracy: 1.0000\n",
      "\"Batch number:2370, Epoch:001, Training: Loss: 0.0061 , Accuracy: 1.0000\n",
      "\"Batch number:2380, Epoch:001, Training: Loss: 0.0290 , Accuracy: 1.0000\n",
      "\"Batch number:2390, Epoch:001, Training: Loss: 0.0034 , Accuracy: 1.0000\n",
      "\"Batch number:2400, Epoch:001, Training: Loss: 0.0297 , Accuracy: 1.0000\n",
      "\"Batch number:2410, Epoch:001, Training: Loss: 0.0077 , Accuracy: 1.0000\n",
      "\"Batch number:2420, Epoch:001, Training: Loss: 0.0026 , Accuracy: 1.0000\n",
      "\"Batch number:2430, Epoch:001, Training: Loss: 0.9733 , Accuracy: 0.7500\n",
      "\"Batch number:2440, Epoch:001, Training: Loss: 0.0026 , Accuracy: 1.0000\n",
      "\"Batch number:2450, Epoch:001, Training: Loss: 0.0205 , Accuracy: 1.0000\n",
      "\"Batch number:2460, Epoch:001, Training: Loss: 0.1925 , Accuracy: 1.0000\n",
      "\"Batch number:2470, Epoch:001, Training: Loss: 0.0314 , Accuracy: 1.0000\n",
      "\"Batch number:2480, Epoch:001, Training: Loss: 0.0108 , Accuracy: 1.0000\n",
      "\"Batch number:2490, Epoch:001, Training: Loss: 0.0173 , Accuracy: 1.0000\n",
      "\"Batch number:2500, Epoch:001, Training: Loss: 0.0088 , Accuracy: 1.0000\n",
      "\"Batch number:2510, Epoch:001, Training: Loss: 0.0213 , Accuracy: 1.0000\n",
      "\"Batch number:2520, Epoch:001, Training: Loss: 0.2492 , Accuracy: 1.0000\n",
      "\"Batch number:2530, Epoch:001, Training: Loss: 0.0394 , Accuracy: 1.0000\n",
      "\"Batch number:2540, Epoch:001, Training: Loss: 0.0043 , Accuracy: 1.0000\n",
      "\"Batch number:2550, Epoch:001, Training: Loss: 0.0033 , Accuracy: 1.0000\n",
      "\"Batch number:2560, Epoch:001, Training: Loss: 0.0200 , Accuracy: 1.0000\n",
      "\"Batch number:2570, Epoch:001, Training: Loss: 0.0023 , Accuracy: 1.0000\n",
      "\"Batch number:2580, Epoch:001, Training: Loss: 0.0364 , Accuracy: 1.0000\n",
      "\"Batch number:2590, Epoch:001, Training: Loss: 0.0031 , Accuracy: 1.0000\n",
      "\"Batch number:2600, Epoch:001, Training: Loss: 0.0068 , Accuracy: 1.0000\n",
      "\"Batch number:2610, Epoch:001, Training: Loss: 0.0521 , Accuracy: 1.0000\n",
      "\"Batch number:2620, Epoch:001, Training: Loss: 0.0029 , Accuracy: 1.0000\n",
      "\"Batch number:2630, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:2640, Epoch:001, Training: Loss: 0.0857 , Accuracy: 1.0000\n",
      "\"Batch number:2650, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:2660, Epoch:001, Training: Loss: 0.0007 , Accuracy: 1.0000\n",
      "\"Batch number:2670, Epoch:001, Training: Loss: 0.2084 , Accuracy: 1.0000\n",
      "\"Batch number:2680, Epoch:001, Training: Loss: 0.0683 , Accuracy: 1.0000\n",
      "\"Batch number:2690, Epoch:001, Training: Loss: 0.0440 , Accuracy: 1.0000\n",
      "\"Batch number:2700, Epoch:001, Training: Loss: 0.0143 , Accuracy: 1.0000\n",
      "\"Batch number:2710, Epoch:001, Training: Loss: 0.0012 , Accuracy: 1.0000\n",
      "\"Batch number:2720, Epoch:001, Training: Loss: 0.0083 , Accuracy: 1.0000\n",
      "\"Batch number:2730, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:2740, Epoch:001, Training: Loss: 0.0065 , Accuracy: 1.0000\n",
      "\"Batch number:2750, Epoch:001, Training: Loss: 0.0076 , Accuracy: 1.0000\n",
      "\"Batch number:2760, Epoch:001, Training: Loss: 0.0062 , Accuracy: 1.0000\n",
      "\"Batch number:2770, Epoch:001, Training: Loss: 0.0190 , Accuracy: 1.0000\n",
      "\"Batch number:2780, Epoch:001, Training: Loss: 0.0027 , Accuracy: 1.0000\n",
      "\"Batch number:2790, Epoch:001, Training: Loss: 0.0044 , Accuracy: 1.0000\n",
      "\"Batch number:2800, Epoch:001, Training: Loss: 0.0043 , Accuracy: 1.0000\n",
      "\"Batch number:2810, Epoch:001, Training: Loss: 0.0347 , Accuracy: 1.0000\n",
      "\"Batch number:2820, Epoch:001, Training: Loss: 0.0046 , Accuracy: 1.0000\n",
      "\"Batch number:2830, Epoch:001, Training: Loss: 0.0252 , Accuracy: 1.0000\n",
      "\"Batch number:2840, Epoch:001, Training: Loss: 0.0753 , Accuracy: 1.0000\n",
      "\"Batch number:2850, Epoch:001, Training: Loss: 0.0949 , Accuracy: 1.0000\n",
      "\"Batch number:2860, Epoch:001, Training: Loss: 0.1075 , Accuracy: 1.0000\n",
      "\"Batch number:2870, Epoch:001, Training: Loss: 0.0455 , Accuracy: 1.0000\n",
      "\"Batch number:2880, Epoch:001, Training: Loss: 0.0106 , Accuracy: 1.0000\n",
      "\"Batch number:2890, Epoch:001, Training: Loss: 0.0013 , Accuracy: 1.0000\n",
      "\"Batch number:2900, Epoch:001, Training: Loss: 0.0229 , Accuracy: 1.0000\n",
      "\"Batch number:2910, Epoch:001, Training: Loss: 0.7527 , Accuracy: 0.7500\n",
      "\"Batch number:2920, Epoch:001, Training: Loss: 0.2164 , Accuracy: 1.0000\n",
      "\"Batch number:2930, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:2940, Epoch:001, Training: Loss: 0.6820 , Accuracy: 0.7500\n",
      "\"Batch number:2950, Epoch:001, Training: Loss: 0.0115 , Accuracy: 1.0000\n",
      "\"Batch number:2960, Epoch:001, Training: Loss: 0.0067 , Accuracy: 1.0000\n",
      "\"Batch number:2970, Epoch:001, Training: Loss: 1.0506 , Accuracy: 0.7500\n",
      "\"Batch number:2980, Epoch:001, Training: Loss: 0.0023 , Accuracy: 1.0000\n",
      "\"Batch number:2990, Epoch:001, Training: Loss: 0.0067 , Accuracy: 1.0000\n",
      "\"Batch number:3000, Epoch:001, Training: Loss: 0.0052 , Accuracy: 1.0000\n",
      "\"Batch number:3010, Epoch:001, Training: Loss: 0.0082 , Accuracy: 1.0000\n",
      "\"Batch number:3020, Epoch:001, Training: Loss: 0.0018 , Accuracy: 1.0000\n",
      "\"Batch number:3030, Epoch:001, Training: Loss: 0.0019 , Accuracy: 1.0000\n",
      "\"Batch number:3040, Epoch:001, Training: Loss: 0.0059 , Accuracy: 1.0000\n",
      "\"Batch number:3050, Epoch:001, Training: Loss: 0.0066 , Accuracy: 1.0000\n",
      "\"Batch number:3060, Epoch:001, Training: Loss: 0.0020 , Accuracy: 1.0000\n",
      "\"Batch number:3070, Epoch:001, Training: Loss: 0.0240 , Accuracy: 1.0000\n",
      "\"Batch number:3080, Epoch:001, Training: Loss: 0.0013 , Accuracy: 1.0000\n",
      "\"Batch number:3090, Epoch:001, Training: Loss: 0.0049 , Accuracy: 1.0000\n",
      "\"Batch number:3100, Epoch:001, Training: Loss: 0.0091 , Accuracy: 1.0000\n",
      "\"Batch number:3110, Epoch:001, Training: Loss: 0.0244 , Accuracy: 1.0000\n",
      "\"Batch number:3120, Epoch:001, Training: Loss: 0.0006 , Accuracy: 1.0000\n",
      "\"Batch number:3130, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n",
      "\"Batch number:3140, Epoch:001, Training: Loss: 0.0097 , Accuracy: 1.0000\n",
      "\"Batch number:3150, Epoch:001, Training: Loss: 0.0059 , Accuracy: 1.0000\n",
      "\"Batch number:3160, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:3170, Epoch:001, Training: Loss: 0.0076 , Accuracy: 1.0000\n",
      "\"Batch number:3180, Epoch:001, Training: Loss: 0.0295 , Accuracy: 1.0000\n",
      "\"Batch number:3190, Epoch:001, Training: Loss: 0.0216 , Accuracy: 1.0000\n",
      "\"Batch number:3200, Epoch:001, Training: Loss: 0.0007 , Accuracy: 1.0000\n",
      "\"Batch number:3210, Epoch:001, Training: Loss: 0.0021 , Accuracy: 1.0000\n",
      "\"Batch number:3220, Epoch:001, Training: Loss: 0.0152 , Accuracy: 1.0000\n",
      "\"Batch number:3230, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:3240, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:3250, Epoch:001, Training: Loss: 0.0201 , Accuracy: 1.0000\n",
      "\"Batch number:3260, Epoch:001, Training: Loss: 0.4236 , Accuracy: 0.7500\n",
      "\"Batch number:3270, Epoch:001, Training: Loss: 0.0112 , Accuracy: 1.0000\n",
      "\"Batch number:3280, Epoch:001, Training: Loss: 0.0190 , Accuracy: 1.0000\n",
      "\"Batch number:3290, Epoch:001, Training: Loss: 0.0365 , Accuracy: 1.0000\n",
      "\"Batch number:3300, Epoch:001, Training: Loss: 0.5288 , Accuracy: 0.7500\n",
      "\"Batch number:3310, Epoch:001, Training: Loss: 0.0245 , Accuracy: 1.0000\n",
      "\"Batch number:3320, Epoch:001, Training: Loss: 0.0073 , Accuracy: 1.0000\n",
      "\"Batch number:3330, Epoch:001, Training: Loss: 0.0080 , Accuracy: 1.0000\n",
      "\"Batch number:3340, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n",
      "\"Batch number:3350, Epoch:001, Training: Loss: 0.0023 , Accuracy: 1.0000\n",
      "\"Batch number:3360, Epoch:001, Training: Loss: 0.0217 , Accuracy: 1.0000\n",
      "\"Batch number:3370, Epoch:001, Training: Loss: 0.0021 , Accuracy: 1.0000\n",
      "\"Batch number:3380, Epoch:001, Training: Loss: 0.0129 , Accuracy: 1.0000\n",
      "\"Batch number:3390, Epoch:001, Training: Loss: 0.0382 , Accuracy: 1.0000\n",
      "\"Batch number:3400, Epoch:001, Training: Loss: 0.0092 , Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:3410, Epoch:001, Training: Loss: 0.0036 , Accuracy: 1.0000\n",
      "\"Batch number:3420, Epoch:001, Training: Loss: 0.0096 , Accuracy: 1.0000\n",
      "\"Batch number:3430, Epoch:001, Training: Loss: 0.0043 , Accuracy: 1.0000\n",
      "\"Batch number:3440, Epoch:001, Training: Loss: 0.0064 , Accuracy: 1.0000\n",
      "\"Batch number:3450, Epoch:001, Training: Loss: 0.0033 , Accuracy: 1.0000\n",
      "\"Batch number:3460, Epoch:001, Training: Loss: 0.0132 , Accuracy: 1.0000\n",
      "\"Batch number:3470, Epoch:001, Training: Loss: 0.0071 , Accuracy: 1.0000\n",
      "\"Batch number:3480, Epoch:001, Training: Loss: 0.0093 , Accuracy: 1.0000\n",
      "\"Batch number:3490, Epoch:001, Training: Loss: 0.0216 , Accuracy: 1.0000\n",
      "\"Batch number:3500, Epoch:001, Training: Loss: 0.0026 , Accuracy: 1.0000\n",
      "\"Batch number:3510, Epoch:001, Training: Loss: 0.0031 , Accuracy: 1.0000\n",
      "\"Batch number:3520, Epoch:001, Training: Loss: 0.1364 , Accuracy: 1.0000\n",
      "\"Batch number:3530, Epoch:001, Training: Loss: 0.0120 , Accuracy: 1.0000\n",
      "\"Batch number:3540, Epoch:001, Training: Loss: 0.3728 , Accuracy: 0.7500\n",
      "\"Batch number:3550, Epoch:001, Training: Loss: 0.0304 , Accuracy: 1.0000\n",
      "\"Batch number:3560, Epoch:001, Training: Loss: 0.0126 , Accuracy: 1.0000\n",
      "\"Batch number:3570, Epoch:001, Training: Loss: 0.0152 , Accuracy: 1.0000\n",
      "\"Batch number:3580, Epoch:001, Training: Loss: 0.1862 , Accuracy: 1.0000\n",
      "\"Batch number:3590, Epoch:001, Training: Loss: 0.0551 , Accuracy: 1.0000\n",
      "\"Batch number:3600, Epoch:001, Training: Loss: 0.1231 , Accuracy: 1.0000\n",
      "\"Batch number:3610, Epoch:001, Training: Loss: 0.0337 , Accuracy: 1.0000\n",
      "\"Batch number:3620, Epoch:001, Training: Loss: 0.0031 , Accuracy: 1.0000\n",
      "\"Batch number:3630, Epoch:001, Training: Loss: 1.4206 , Accuracy: 0.7500\n",
      "\"Batch number:3640, Epoch:001, Training: Loss: 0.0054 , Accuracy: 1.0000\n",
      "\"Batch number:3650, Epoch:001, Training: Loss: 0.1316 , Accuracy: 1.0000\n",
      "\"Batch number:3660, Epoch:001, Training: Loss: 0.1051 , Accuracy: 1.0000\n",
      "\"Batch number:3670, Epoch:001, Training: Loss: 0.0152 , Accuracy: 1.0000\n",
      "\"Batch number:3680, Epoch:001, Training: Loss: 0.0035 , Accuracy: 1.0000\n",
      "\"Batch number:3690, Epoch:001, Training: Loss: 0.5453 , Accuracy: 0.7500\n",
      "\"Batch number:3700, Epoch:001, Training: Loss: 0.0646 , Accuracy: 1.0000\n",
      "\"Batch number:3710, Epoch:001, Training: Loss: 0.0239 , Accuracy: 1.0000\n",
      "\"Batch number:3720, Epoch:001, Training: Loss: 0.0062 , Accuracy: 1.0000\n",
      "\"Batch number:3730, Epoch:001, Training: Loss: 0.6822 , Accuracy: 0.7500\n",
      "\"Batch number:3740, Epoch:001, Training: Loss: 0.0611 , Accuracy: 1.0000\n",
      "\"Batch number:3750, Epoch:001, Training: Loss: 0.0055 , Accuracy: 1.0000\n",
      "\"Batch number:3760, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:3770, Epoch:001, Training: Loss: 0.1421 , Accuracy: 1.0000\n",
      "\"Batch number:3780, Epoch:001, Training: Loss: 0.0029 , Accuracy: 1.0000\n",
      "\"Batch number:3790, Epoch:001, Training: Loss: 0.0011 , Accuracy: 1.0000\n",
      "\"Batch number:3800, Epoch:001, Training: Loss: 0.0015 , Accuracy: 1.0000\n",
      "\"Batch number:3810, Epoch:001, Training: Loss: 0.0041 , Accuracy: 1.0000\n",
      "\"Batch number:3820, Epoch:001, Training: Loss: 0.0009 , Accuracy: 1.0000\n",
      "\"Batch number:3830, Epoch:001, Training: Loss: 0.0060 , Accuracy: 1.0000\n",
      "\"Batch number:3840, Epoch:001, Training: Loss: 0.0012 , Accuracy: 1.0000\n",
      "\"Batch number:3850, Epoch:001, Training: Loss: 0.0036 , Accuracy: 1.0000\n",
      "\"Batch number:3860, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:3870, Epoch:001, Training: Loss: 0.0345 , Accuracy: 1.0000\n",
      "\"Batch number:3880, Epoch:001, Training: Loss: 0.0059 , Accuracy: 1.0000\n",
      "\"Batch number:3890, Epoch:001, Training: Loss: 0.4299 , Accuracy: 0.7500\n",
      "\"Batch number:3900, Epoch:001, Training: Loss: 0.0040 , Accuracy: 1.0000\n",
      "\"Batch number:3910, Epoch:001, Training: Loss: 0.0080 , Accuracy: 1.0000\n",
      "\"Batch number:3920, Epoch:001, Training: Loss: 0.1994 , Accuracy: 1.0000\n",
      "\"Batch number:3930, Epoch:001, Training: Loss: 0.0532 , Accuracy: 1.0000\n",
      "\"Batch number:3940, Epoch:001, Training: Loss: 0.0015 , Accuracy: 1.0000\n",
      "\"Batch number:3950, Epoch:001, Training: Loss: 0.0097 , Accuracy: 1.0000\n",
      "\"Batch number:3960, Epoch:001, Training: Loss: 0.0314 , Accuracy: 1.0000\n",
      "\"Batch number:3970, Epoch:001, Training: Loss: 0.0012 , Accuracy: 1.0000\n",
      "\"Batch number:3980, Epoch:001, Training: Loss: 0.0028 , Accuracy: 1.0000\n",
      "\"Batch number:3990, Epoch:001, Training: Loss: 0.0143 , Accuracy: 1.0000\n",
      "\"Batch number:4000, Epoch:001, Training: Loss: 0.0019 , Accuracy: 1.0000\n",
      "\"Batch number:4010, Epoch:001, Training: Loss: 0.0064 , Accuracy: 1.0000\n",
      "\"Batch number:4020, Epoch:001, Training: Loss: 0.0075 , Accuracy: 1.0000\n",
      "\"Batch number:4030, Epoch:001, Training: Loss: 0.0049 , Accuracy: 1.0000\n",
      "\"Batch number:4040, Epoch:001, Training: Loss: 0.0171 , Accuracy: 1.0000\n",
      "\"Batch number:4050, Epoch:001, Training: Loss: 8.5949 , Accuracy: 0.0000\n",
      "\"Batch number:4060, Epoch:001, Training: Loss: 2.8851 , Accuracy: 0.2500\n",
      "\"Batch number:4070, Epoch:001, Training: Loss: 1.4385 , Accuracy: 0.7500\n",
      "\"Batch number:4080, Epoch:001, Training: Loss: 1.2324 , Accuracy: 0.5000\n",
      "\"Batch number:4090, Epoch:001, Training: Loss: 1.8598 , Accuracy: 0.5000\n",
      "\"Batch number:4100, Epoch:001, Training: Loss: 1.7447 , Accuracy: 0.5000\n",
      "\"Batch number:4110, Epoch:001, Training: Loss: 2.0582 , Accuracy: 0.2500\n",
      "\"Batch number:4120, Epoch:001, Training: Loss: 1.8386 , Accuracy: 0.5000\n",
      "\"Batch number:4130, Epoch:001, Training: Loss: 1.2158 , Accuracy: 0.7500\n",
      "\"Batch number:4140, Epoch:001, Training: Loss: 2.1317 , Accuracy: 0.5000\n",
      "\"Batch number:4150, Epoch:001, Training: Loss: 1.5256 , Accuracy: 0.5000\n",
      "\"Batch number:4160, Epoch:001, Training: Loss: 1.7734 , Accuracy: 0.5000\n",
      "\"Batch number:4170, Epoch:001, Training: Loss: 0.3566 , Accuracy: 1.0000\n",
      "\"Batch number:4180, Epoch:001, Training: Loss: 0.8540 , Accuracy: 0.7500\n",
      "\"Batch number:4190, Epoch:001, Training: Loss: 1.6011 , Accuracy: 0.7500\n",
      "\"Batch number:4200, Epoch:001, Training: Loss: 3.1735 , Accuracy: 0.0000\n",
      "\"Batch number:4210, Epoch:001, Training: Loss: 0.8152 , Accuracy: 0.7500\n",
      "\"Batch number:4220, Epoch:001, Training: Loss: 2.5909 , Accuracy: 0.0000\n",
      "\"Batch number:4230, Epoch:001, Training: Loss: 1.7479 , Accuracy: 0.5000\n",
      "\"Batch number:4240, Epoch:001, Training: Loss: 0.8003 , Accuracy: 0.7500\n",
      "\"Batch number:4250, Epoch:001, Training: Loss: 1.8712 , Accuracy: 0.2500\n",
      "\"Batch number:4260, Epoch:001, Training: Loss: 0.2789 , Accuracy: 1.0000\n",
      "\"Batch number:4270, Epoch:001, Training: Loss: 0.3779 , Accuracy: 1.0000\n",
      "\"Batch number:4280, Epoch:001, Training: Loss: 0.1834 , Accuracy: 1.0000\n",
      "\"Batch number:4290, Epoch:001, Training: Loss: 0.2273 , Accuracy: 1.0000\n",
      "\"Batch number:4300, Epoch:001, Training: Loss: 0.1372 , Accuracy: 1.0000\n",
      "\"Batch number:4310, Epoch:001, Training: Loss: 0.4768 , Accuracy: 1.0000\n",
      "\"Batch number:4320, Epoch:001, Training: Loss: 1.9438 , Accuracy: 0.7500\n",
      "\"Batch number:4330, Epoch:001, Training: Loss: 0.0847 , Accuracy: 1.0000\n",
      "\"Batch number:4340, Epoch:001, Training: Loss: 0.6931 , Accuracy: 0.7500\n",
      "\"Batch number:4350, Epoch:001, Training: Loss: 0.0500 , Accuracy: 1.0000\n",
      "\"Batch number:4360, Epoch:001, Training: Loss: 0.2004 , Accuracy: 1.0000\n",
      "\"Batch number:4370, Epoch:001, Training: Loss: 0.2980 , Accuracy: 1.0000\n",
      "\"Batch number:4380, Epoch:001, Training: Loss: 0.5356 , Accuracy: 0.7500\n",
      "\"Batch number:4390, Epoch:001, Training: Loss: 0.4233 , Accuracy: 1.0000\n",
      "\"Batch number:4400, Epoch:001, Training: Loss: 0.1424 , Accuracy: 1.0000\n",
      "\"Batch number:4410, Epoch:001, Training: Loss: 1.2032 , Accuracy: 0.5000\n",
      "\"Batch number:4420, Epoch:001, Training: Loss: 0.0332 , Accuracy: 1.0000\n",
      "\"Batch number:4430, Epoch:001, Training: Loss: 0.7061 , Accuracy: 0.7500\n",
      "\"Batch number:4440, Epoch:001, Training: Loss: 0.0514 , Accuracy: 1.0000\n",
      "\"Batch number:4450, Epoch:001, Training: Loss: 0.7933 , Accuracy: 0.7500\n",
      "\"Batch number:4460, Epoch:001, Training: Loss: 0.1601 , Accuracy: 1.0000\n",
      "\"Batch number:4470, Epoch:001, Training: Loss: 0.0807 , Accuracy: 1.0000\n",
      "\"Batch number:4480, Epoch:001, Training: Loss: 0.6273 , Accuracy: 0.7500\n",
      "\"Batch number:4490, Epoch:001, Training: Loss: 0.0374 , Accuracy: 1.0000\n",
      "\"Batch number:4500, Epoch:001, Training: Loss: 0.0479 , Accuracy: 1.0000\n",
      "\"Batch number:4510, Epoch:001, Training: Loss: 0.5536 , Accuracy: 0.7500\n",
      "\"Batch number:4520, Epoch:001, Training: Loss: 0.1932 , Accuracy: 1.0000\n",
      "\"Batch number:4530, Epoch:001, Training: Loss: 0.1963 , Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:4540, Epoch:001, Training: Loss: 0.1501 , Accuracy: 1.0000\n",
      "\"Batch number:4550, Epoch:001, Training: Loss: 0.3306 , Accuracy: 0.7500\n",
      "\"Batch number:4560, Epoch:001, Training: Loss: 0.3208 , Accuracy: 0.7500\n",
      "\"Batch number:4570, Epoch:001, Training: Loss: 0.0453 , Accuracy: 1.0000\n",
      "\"Batch number:4580, Epoch:001, Training: Loss: 0.0755 , Accuracy: 1.0000\n",
      "\"Batch number:4590, Epoch:001, Training: Loss: 0.4000 , Accuracy: 0.7500\n",
      "\"Batch number:4600, Epoch:001, Training: Loss: 0.0734 , Accuracy: 1.0000\n",
      "\"Batch number:4610, Epoch:001, Training: Loss: 2.9556 , Accuracy: 0.2500\n",
      "\"Batch number:4620, Epoch:001, Training: Loss: 0.0418 , Accuracy: 1.0000\n",
      "\"Batch number:4630, Epoch:001, Training: Loss: 1.0366 , Accuracy: 0.5000\n",
      "\"Batch number:4640, Epoch:001, Training: Loss: 0.9021 , Accuracy: 0.7500\n",
      "\"Batch number:4650, Epoch:001, Training: Loss: 0.0171 , Accuracy: 1.0000\n",
      "\"Batch number:4660, Epoch:001, Training: Loss: 0.0626 , Accuracy: 1.0000\n",
      "\"Batch number:4670, Epoch:001, Training: Loss: 0.0541 , Accuracy: 1.0000\n",
      "\"Batch number:4680, Epoch:001, Training: Loss: 0.9333 , Accuracy: 0.7500\n",
      "\"Batch number:4690, Epoch:001, Training: Loss: 0.0135 , Accuracy: 1.0000\n",
      "\"Batch number:4700, Epoch:001, Training: Loss: 0.1981 , Accuracy: 1.0000\n",
      "\"Batch number:4710, Epoch:001, Training: Loss: 0.7924 , Accuracy: 0.7500\n",
      "\"Batch number:4720, Epoch:001, Training: Loss: 0.0222 , Accuracy: 1.0000\n",
      "\"Batch number:4730, Epoch:001, Training: Loss: 0.0910 , Accuracy: 1.0000\n",
      "\"Batch number:4740, Epoch:001, Training: Loss: 0.2428 , Accuracy: 1.0000\n",
      "\"Batch number:4750, Epoch:001, Training: Loss: 0.2027 , Accuracy: 1.0000\n",
      "\"Batch number:4760, Epoch:001, Training: Loss: 0.0123 , Accuracy: 1.0000\n",
      "\"Batch number:4770, Epoch:001, Training: Loss: 0.4345 , Accuracy: 1.0000\n",
      "\"Batch number:4780, Epoch:001, Training: Loss: 0.0184 , Accuracy: 1.0000\n",
      "\"Batch number:4790, Epoch:001, Training: Loss: 0.0356 , Accuracy: 1.0000\n",
      "\"Batch number:4800, Epoch:001, Training: Loss: 0.1037 , Accuracy: 1.0000\n",
      "\"Batch number:4810, Epoch:001, Training: Loss: 0.0623 , Accuracy: 1.0000\n",
      "\"Batch number:4820, Epoch:001, Training: Loss: 0.7319 , Accuracy: 0.7500\n",
      "\"Batch number:4830, Epoch:001, Training: Loss: 0.2687 , Accuracy: 0.7500\n",
      "\"Batch number:4840, Epoch:001, Training: Loss: 0.5623 , Accuracy: 0.7500\n",
      "\"Batch number:4850, Epoch:001, Training: Loss: 0.0978 , Accuracy: 1.0000\n",
      "\"Batch number:4860, Epoch:001, Training: Loss: 0.2407 , Accuracy: 1.0000\n",
      "\"Batch number:4870, Epoch:001, Training: Loss: 0.0893 , Accuracy: 1.0000\n",
      "\"Batch number:4880, Epoch:001, Training: Loss: 0.0485 , Accuracy: 1.0000\n",
      "\"Batch number:4890, Epoch:001, Training: Loss: 0.2229 , Accuracy: 1.0000\n",
      "\"Batch number:4900, Epoch:001, Training: Loss: 0.0120 , Accuracy: 1.0000\n",
      "\"Batch number:4910, Epoch:001, Training: Loss: 0.8942 , Accuracy: 0.7500\n",
      "\"Batch number:4920, Epoch:001, Training: Loss: 1.4426 , Accuracy: 0.7500\n",
      "\"Batch number:4930, Epoch:001, Training: Loss: 0.0141 , Accuracy: 1.0000\n",
      "\"Batch number:4940, Epoch:001, Training: Loss: 0.1331 , Accuracy: 1.0000\n",
      "\"Batch number:4950, Epoch:001, Training: Loss: 0.0105 , Accuracy: 1.0000\n",
      "\"Batch number:4960, Epoch:001, Training: Loss: 0.6484 , Accuracy: 0.7500\n",
      "\"Batch number:4970, Epoch:001, Training: Loss: 0.0040 , Accuracy: 1.0000\n",
      "\"Batch number:4980, Epoch:001, Training: Loss: 0.1242 , Accuracy: 1.0000\n",
      "\"Batch number:4990, Epoch:001, Training: Loss: 0.0334 , Accuracy: 1.0000\n",
      "\"Batch number:5000, Epoch:001, Training: Loss: 0.4010 , Accuracy: 1.0000\n",
      "\"Batch number:5010, Epoch:001, Training: Loss: 0.1462 , Accuracy: 1.0000\n",
      "\"Batch number:5020, Epoch:001, Training: Loss: 0.0538 , Accuracy: 1.0000\n",
      "\"Batch number:5030, Epoch:001, Training: Loss: 0.0226 , Accuracy: 1.0000\n",
      "\"Batch number:5040, Epoch:001, Training: Loss: 0.0419 , Accuracy: 1.0000\n",
      "\"Batch number:5050, Epoch:001, Training: Loss: 0.3330 , Accuracy: 1.0000\n",
      "\"Batch number:5060, Epoch:001, Training: Loss: 0.0089 , Accuracy: 1.0000\n",
      "\"Batch number:5070, Epoch:001, Training: Loss: 0.0289 , Accuracy: 1.0000\n",
      "\"Batch number:5080, Epoch:001, Training: Loss: 0.1299 , Accuracy: 1.0000\n",
      "\"Batch number:5090, Epoch:001, Training: Loss: 0.0325 , Accuracy: 1.0000\n",
      "\"Batch number:5100, Epoch:001, Training: Loss: 0.0470 , Accuracy: 1.0000\n",
      "\"Batch number:5110, Epoch:001, Training: Loss: 0.0301 , Accuracy: 1.0000\n",
      "\"Batch number:5120, Epoch:001, Training: Loss: 0.0299 , Accuracy: 1.0000\n",
      "\"Batch number:5130, Epoch:001, Training: Loss: 0.0119 , Accuracy: 1.0000\n",
      "\"Batch number:5140, Epoch:001, Training: Loss: 0.0237 , Accuracy: 1.0000\n",
      "\"Batch number:5150, Epoch:001, Training: Loss: 0.0342 , Accuracy: 1.0000\n",
      "\"Batch number:5160, Epoch:001, Training: Loss: 0.2965 , Accuracy: 1.0000\n",
      "\"Batch number:5170, Epoch:001, Training: Loss: 0.0977 , Accuracy: 1.0000\n",
      "\"Batch number:5180, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:5190, Epoch:001, Training: Loss: 0.0049 , Accuracy: 1.0000\n",
      "\"Batch number:5200, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:5210, Epoch:001, Training: Loss: 0.5130 , Accuracy: 0.7500\n",
      "\"Batch number:5220, Epoch:001, Training: Loss: 0.0757 , Accuracy: 1.0000\n",
      "\"Batch number:5230, Epoch:001, Training: Loss: 0.0509 , Accuracy: 1.0000\n",
      "\"Batch number:5240, Epoch:001, Training: Loss: 0.2463 , Accuracy: 1.0000\n",
      "\"Batch number:5250, Epoch:001, Training: Loss: 0.0441 , Accuracy: 1.0000\n",
      "\"Batch number:5260, Epoch:001, Training: Loss: 0.5076 , Accuracy: 0.7500\n",
      "\"Batch number:5270, Epoch:001, Training: Loss: 0.0190 , Accuracy: 1.0000\n",
      "\"Batch number:5280, Epoch:001, Training: Loss: 0.0108 , Accuracy: 1.0000\n",
      "\"Batch number:5290, Epoch:001, Training: Loss: 0.0123 , Accuracy: 1.0000\n",
      "\"Batch number:5300, Epoch:001, Training: Loss: 0.0108 , Accuracy: 1.0000\n",
      "\"Batch number:5310, Epoch:001, Training: Loss: 0.0090 , Accuracy: 1.0000\n",
      "\"Batch number:5320, Epoch:001, Training: Loss: 0.0868 , Accuracy: 1.0000\n",
      "\"Batch number:5330, Epoch:001, Training: Loss: 0.0771 , Accuracy: 1.0000\n",
      "\"Batch number:5340, Epoch:001, Training: Loss: 0.2473 , Accuracy: 1.0000\n",
      "\"Batch number:5350, Epoch:001, Training: Loss: 0.0133 , Accuracy: 1.0000\n",
      "\"Batch number:5360, Epoch:001, Training: Loss: 0.0488 , Accuracy: 1.0000\n",
      "\"Batch number:5370, Epoch:001, Training: Loss: 0.0137 , Accuracy: 1.0000\n",
      "\"Batch number:5380, Epoch:001, Training: Loss: 0.0538 , Accuracy: 1.0000\n",
      "\"Batch number:5390, Epoch:001, Training: Loss: 0.0060 , Accuracy: 1.0000\n",
      "\"Batch number:5400, Epoch:001, Training: Loss: 0.0038 , Accuracy: 1.0000\n",
      "\"Batch number:5410, Epoch:001, Training: Loss: 0.0432 , Accuracy: 1.0000\n",
      "\"Batch number:5420, Epoch:001, Training: Loss: 0.2197 , Accuracy: 1.0000\n",
      "\"Batch number:5430, Epoch:001, Training: Loss: 0.0103 , Accuracy: 1.0000\n",
      "\"Batch number:5440, Epoch:001, Training: Loss: 0.0141 , Accuracy: 1.0000\n",
      "\"Batch number:5450, Epoch:001, Training: Loss: 0.0075 , Accuracy: 1.0000\n",
      "\"Batch number:5460, Epoch:001, Training: Loss: 0.1986 , Accuracy: 1.0000\n",
      "\"Batch number:5470, Epoch:001, Training: Loss: 0.0586 , Accuracy: 1.0000\n",
      "\"Batch number:5480, Epoch:001, Training: Loss: 0.0222 , Accuracy: 1.0000\n",
      "\"Batch number:5490, Epoch:001, Training: Loss: 0.0108 , Accuracy: 1.0000\n",
      "\"Batch number:5500, Epoch:001, Training: Loss: 0.0089 , Accuracy: 1.0000\n",
      "\"Batch number:5510, Epoch:001, Training: Loss: 0.0047 , Accuracy: 1.0000\n",
      "\"Batch number:5520, Epoch:001, Training: Loss: 0.0045 , Accuracy: 1.0000\n",
      "\"Batch number:5530, Epoch:001, Training: Loss: 0.9115 , Accuracy: 0.7500\n",
      "\"Batch number:5540, Epoch:001, Training: Loss: 0.0277 , Accuracy: 1.0000\n",
      "\"Batch number:5550, Epoch:001, Training: Loss: 0.0142 , Accuracy: 1.0000\n",
      "\"Batch number:5560, Epoch:001, Training: Loss: 0.0053 , Accuracy: 1.0000\n",
      "\"Batch number:5570, Epoch:001, Training: Loss: 0.0144 , Accuracy: 1.0000\n",
      "\"Batch number:5580, Epoch:001, Training: Loss: 0.0958 , Accuracy: 1.0000\n",
      "\"Batch number:5590, Epoch:001, Training: Loss: 0.0034 , Accuracy: 1.0000\n",
      "\"Batch number:5600, Epoch:001, Training: Loss: 0.0232 , Accuracy: 1.0000\n",
      "\"Batch number:5610, Epoch:001, Training: Loss: 0.0047 , Accuracy: 1.0000\n",
      "\"Batch number:5620, Epoch:001, Training: Loss: 0.0232 , Accuracy: 1.0000\n",
      "\"Batch number:5630, Epoch:001, Training: Loss: 0.1908 , Accuracy: 1.0000\n",
      "\"Batch number:5640, Epoch:001, Training: Loss: 0.0036 , Accuracy: 1.0000\n",
      "\"Batch number:5650, Epoch:001, Training: Loss: 0.0044 , Accuracy: 1.0000\n",
      "\"Batch number:5660, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:5670, Epoch:001, Training: Loss: 0.0937 , Accuracy: 1.0000\n",
      "\"Batch number:5680, Epoch:001, Training: Loss: 0.0036 , Accuracy: 1.0000\n",
      "\"Batch number:5690, Epoch:001, Training: Loss: 0.0178 , Accuracy: 1.0000\n",
      "\"Batch number:5700, Epoch:001, Training: Loss: 0.0296 , Accuracy: 1.0000\n",
      "\"Batch number:5710, Epoch:001, Training: Loss: 0.0675 , Accuracy: 1.0000\n",
      "\"Batch number:5720, Epoch:001, Training: Loss: 0.0073 , Accuracy: 1.0000\n",
      "\"Batch number:5730, Epoch:001, Training: Loss: 0.0149 , Accuracy: 1.0000\n",
      "\"Batch number:5740, Epoch:001, Training: Loss: 0.0230 , Accuracy: 1.0000\n",
      "\"Batch number:5750, Epoch:001, Training: Loss: 0.7837 , Accuracy: 0.7500\n",
      "\"Batch number:5760, Epoch:001, Training: Loss: 0.0027 , Accuracy: 1.0000\n",
      "\"Batch number:5770, Epoch:001, Training: Loss: 0.0147 , Accuracy: 1.0000\n",
      "\"Batch number:5780, Epoch:001, Training: Loss: 0.0078 , Accuracy: 1.0000\n",
      "\"Batch number:5790, Epoch:001, Training: Loss: 0.0143 , Accuracy: 1.0000\n",
      "\"Batch number:5800, Epoch:001, Training: Loss: 1.2247 , Accuracy: 0.7500\n",
      "\"Batch number:5810, Epoch:001, Training: Loss: 0.0194 , Accuracy: 1.0000\n",
      "\"Batch number:5820, Epoch:001, Training: Loss: 0.0131 , Accuracy: 1.0000\n",
      "\"Batch number:5830, Epoch:001, Training: Loss: 0.0384 , Accuracy: 1.0000\n",
      "\"Batch number:5840, Epoch:001, Training: Loss: 0.1433 , Accuracy: 1.0000\n",
      "\"Batch number:5850, Epoch:001, Training: Loss: 0.0018 , Accuracy: 1.0000\n",
      "\"Batch number:5860, Epoch:001, Training: Loss: 0.0589 , Accuracy: 1.0000\n",
      "\"Batch number:5870, Epoch:001, Training: Loss: 0.0046 , Accuracy: 1.0000\n",
      "\"Batch number:5880, Epoch:001, Training: Loss: 0.0875 , Accuracy: 1.0000\n",
      "\"Batch number:5890, Epoch:001, Training: Loss: 0.0573 , Accuracy: 1.0000\n",
      "\"Batch number:5900, Epoch:001, Training: Loss: 0.0179 , Accuracy: 1.0000\n",
      "\"Batch number:5910, Epoch:001, Training: Loss: 0.0395 , Accuracy: 1.0000\n",
      "\"Batch number:5920, Epoch:001, Training: Loss: 0.0083 , Accuracy: 1.0000\n",
      "\"Batch number:5930, Epoch:001, Training: Loss: 0.0031 , Accuracy: 1.0000\n",
      "\"Batch number:5940, Epoch:001, Training: Loss: 0.0052 , Accuracy: 1.0000\n",
      "\"Batch number:5950, Epoch:001, Training: Loss: 0.0050 , Accuracy: 1.0000\n",
      "\"Batch number:5960, Epoch:001, Training: Loss: 0.0387 , Accuracy: 1.0000\n",
      "\"Batch number:5970, Epoch:001, Training: Loss: 0.0079 , Accuracy: 1.0000\n",
      "\"Batch number:5980, Epoch:001, Training: Loss: 0.0219 , Accuracy: 1.0000\n",
      "\"Batch number:5990, Epoch:001, Training: Loss: 0.0031 , Accuracy: 1.0000\n",
      "\"Batch number:6000, Epoch:001, Training: Loss: 0.0236 , Accuracy: 1.0000\n",
      "\"Batch number:6010, Epoch:001, Training: Loss: 0.0009 , Accuracy: 1.0000\n",
      "\"Batch number:6020, Epoch:001, Training: Loss: 0.0278 , Accuracy: 1.0000\n",
      "\"Batch number:6030, Epoch:001, Training: Loss: 0.0149 , Accuracy: 1.0000\n",
      "\"Batch number:6040, Epoch:001, Training: Loss: 1.0320 , Accuracy: 0.7500\n",
      "\"Batch number:6050, Epoch:001, Training: Loss: 0.3706 , Accuracy: 1.0000\n",
      "\"Batch number:6060, Epoch:001, Training: Loss: 0.5680 , Accuracy: 0.7500\n",
      "\"Batch number:6070, Epoch:001, Training: Loss: 0.0540 , Accuracy: 1.0000\n",
      "\"Batch number:6080, Epoch:001, Training: Loss: 0.0031 , Accuracy: 1.0000\n",
      "\"Batch number:6090, Epoch:001, Training: Loss: 0.0029 , Accuracy: 1.0000\n",
      "\"Batch number:6100, Epoch:001, Training: Loss: 0.0052 , Accuracy: 1.0000\n",
      "\"Batch number:6110, Epoch:001, Training: Loss: 0.3202 , Accuracy: 1.0000\n",
      "\"Batch number:6120, Epoch:001, Training: Loss: 0.0062 , Accuracy: 1.0000\n",
      "\"Batch number:6130, Epoch:001, Training: Loss: 0.0477 , Accuracy: 1.0000\n",
      "\"Batch number:6140, Epoch:001, Training: Loss: 0.0069 , Accuracy: 1.0000\n",
      "\"Batch number:6150, Epoch:001, Training: Loss: 0.0125 , Accuracy: 1.0000\n",
      "\"Batch number:6160, Epoch:001, Training: Loss: 0.0251 , Accuracy: 1.0000\n",
      "\"Batch number:6170, Epoch:001, Training: Loss: 0.0011 , Accuracy: 1.0000\n",
      "\"Batch number:6180, Epoch:001, Training: Loss: 0.0047 , Accuracy: 1.0000\n",
      "\"Batch number:6190, Epoch:001, Training: Loss: 0.0341 , Accuracy: 1.0000\n",
      "\"Batch number:6200, Epoch:001, Training: Loss: 0.0452 , Accuracy: 1.0000\n",
      "\"Batch number:6210, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n",
      "\"Batch number:6220, Epoch:001, Training: Loss: 0.0123 , Accuracy: 1.0000\n",
      "\"Batch number:6230, Epoch:001, Training: Loss: 0.1484 , Accuracy: 1.0000\n",
      "\"Batch number:6240, Epoch:001, Training: Loss: 0.3494 , Accuracy: 1.0000\n",
      "\"Batch number:6250, Epoch:001, Training: Loss: 0.0043 , Accuracy: 1.0000\n",
      "\"Batch number:6260, Epoch:001, Training: Loss: 0.0038 , Accuracy: 1.0000\n",
      "\"Batch number:6270, Epoch:001, Training: Loss: 0.0079 , Accuracy: 1.0000\n",
      "\"Batch number:6280, Epoch:001, Training: Loss: 0.0028 , Accuracy: 1.0000\n",
      "\"Batch number:6290, Epoch:001, Training: Loss: 0.1015 , Accuracy: 1.0000\n",
      "\"Batch number:6300, Epoch:001, Training: Loss: 0.1043 , Accuracy: 1.0000\n",
      "\"Batch number:6310, Epoch:001, Training: Loss: 0.1206 , Accuracy: 1.0000\n",
      "\"Batch number:6320, Epoch:001, Training: Loss: 0.0039 , Accuracy: 1.0000\n",
      "\"Batch number:6330, Epoch:001, Training: Loss: 0.1108 , Accuracy: 1.0000\n",
      "\"Batch number:6340, Epoch:001, Training: Loss: 0.0096 , Accuracy: 1.0000\n",
      "\"Batch number:6350, Epoch:001, Training: Loss: 0.0038 , Accuracy: 1.0000\n",
      "\"Batch number:6360, Epoch:001, Training: Loss: 0.0129 , Accuracy: 1.0000\n",
      "\"Batch number:6370, Epoch:001, Training: Loss: 0.0056 , Accuracy: 1.0000\n",
      "\"Batch number:6380, Epoch:001, Training: Loss: 0.0109 , Accuracy: 1.0000\n",
      "\"Batch number:6390, Epoch:001, Training: Loss: 0.0380 , Accuracy: 1.0000\n",
      "\"Batch number:6400, Epoch:001, Training: Loss: 0.0098 , Accuracy: 1.0000\n",
      "\"Batch number:6410, Epoch:001, Training: Loss: 0.0056 , Accuracy: 1.0000\n",
      "\"Batch number:6420, Epoch:001, Training: Loss: 0.0113 , Accuracy: 1.0000\n",
      "\"Batch number:6430, Epoch:001, Training: Loss: 0.1308 , Accuracy: 1.0000\n",
      "\"Batch number:6440, Epoch:001, Training: Loss: 0.0041 , Accuracy: 1.0000\n",
      "\"Batch number:6450, Epoch:001, Training: Loss: 0.0147 , Accuracy: 1.0000\n",
      "\"Batch number:6460, Epoch:001, Training: Loss: 0.0014 , Accuracy: 1.0000\n",
      "\"Batch number:6470, Epoch:001, Training: Loss: 0.0332 , Accuracy: 1.0000\n",
      "\"Batch number:6480, Epoch:001, Training: Loss: 0.0528 , Accuracy: 1.0000\n",
      "\"Batch number:6490, Epoch:001, Training: Loss: 0.0028 , Accuracy: 1.0000\n",
      "\"Batch number:6500, Epoch:001, Training: Loss: 0.0352 , Accuracy: 1.0000\n",
      "\"Batch number:6510, Epoch:001, Training: Loss: 0.1598 , Accuracy: 1.0000\n",
      "\"Batch number:6520, Epoch:001, Training: Loss: 0.3049 , Accuracy: 0.7500\n",
      "\"Batch number:6530, Epoch:001, Training: Loss: 0.0045 , Accuracy: 1.0000\n",
      "\"Batch number:6540, Epoch:001, Training: Loss: 0.1978 , Accuracy: 0.7500\n",
      "\"Batch number:6550, Epoch:001, Training: Loss: 0.0457 , Accuracy: 1.0000\n",
      "\"Batch number:6560, Epoch:001, Training: Loss: 0.0335 , Accuracy: 1.0000\n",
      "\"Batch number:6570, Epoch:001, Training: Loss: 0.0056 , Accuracy: 1.0000\n",
      "\"Batch number:6580, Epoch:001, Training: Loss: 0.7946 , Accuracy: 0.7500\n",
      "\"Batch number:6590, Epoch:001, Training: Loss: 0.0459 , Accuracy: 1.0000\n",
      "\"Batch number:6600, Epoch:001, Training: Loss: 0.0010 , Accuracy: 1.0000\n",
      "\"Batch number:6610, Epoch:001, Training: Loss: 0.0231 , Accuracy: 1.0000\n",
      "\"Batch number:6620, Epoch:001, Training: Loss: 0.0172 , Accuracy: 1.0000\n",
      "\"Batch number:6630, Epoch:001, Training: Loss: 0.0164 , Accuracy: 1.0000\n",
      "\"Batch number:6640, Epoch:001, Training: Loss: 0.0061 , Accuracy: 1.0000\n",
      "\"Batch number:6650, Epoch:001, Training: Loss: 0.0030 , Accuracy: 1.0000\n",
      "\"Batch number:6660, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:6670, Epoch:001, Training: Loss: 0.0194 , Accuracy: 1.0000\n",
      "\"Batch number:6680, Epoch:001, Training: Loss: 0.0078 , Accuracy: 1.0000\n",
      "\"Batch number:6690, Epoch:001, Training: Loss: 0.0287 , Accuracy: 1.0000\n",
      "\"Batch number:6700, Epoch:001, Training: Loss: 0.0743 , Accuracy: 1.0000\n",
      "\"Batch number:6710, Epoch:001, Training: Loss: 0.0021 , Accuracy: 1.0000\n",
      "\"Batch number:6720, Epoch:001, Training: Loss: 0.0093 , Accuracy: 1.0000\n",
      "\"Batch number:6730, Epoch:001, Training: Loss: 0.0038 , Accuracy: 1.0000\n",
      "\"Batch number:6740, Epoch:001, Training: Loss: 0.0026 , Accuracy: 1.0000\n",
      "\"Batch number:6750, Epoch:001, Training: Loss: 0.0407 , Accuracy: 1.0000\n",
      "\"Batch number:6760, Epoch:001, Training: Loss: 0.0006 , Accuracy: 1.0000\n",
      "\"Batch number:6770, Epoch:001, Training: Loss: 0.0052 , Accuracy: 1.0000\n",
      "\"Batch number:6780, Epoch:001, Training: Loss: 0.2406 , Accuracy: 1.0000\n",
      "\"Batch number:6790, Epoch:001, Training: Loss: 0.0206 , Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:6800, Epoch:001, Training: Loss: 0.0069 , Accuracy: 1.0000\n",
      "\"Batch number:6810, Epoch:001, Training: Loss: 0.0048 , Accuracy: 1.0000\n",
      "\"Batch number:6820, Epoch:001, Training: Loss: 0.0055 , Accuracy: 1.0000\n",
      "\"Batch number:6830, Epoch:001, Training: Loss: 0.0017 , Accuracy: 1.0000\n",
      "\"Batch number:6840, Epoch:001, Training: Loss: 0.0765 , Accuracy: 1.0000\n",
      "\"Batch number:6850, Epoch:001, Training: Loss: 0.0054 , Accuracy: 1.0000\n",
      "\"Batch number:6860, Epoch:001, Training: Loss: 0.0009 , Accuracy: 1.0000\n",
      "\"Batch number:6870, Epoch:001, Training: Loss: 0.0005 , Accuracy: 1.0000\n",
      "\"Batch number:6880, Epoch:001, Training: Loss: 0.0018 , Accuracy: 1.0000\n",
      "\"Batch number:6890, Epoch:001, Training: Loss: 0.0026 , Accuracy: 1.0000\n",
      "\"Batch number:6900, Epoch:001, Training: Loss: 0.0184 , Accuracy: 1.0000\n",
      "\"Batch number:6910, Epoch:001, Training: Loss: 0.0015 , Accuracy: 1.0000\n",
      "\"Batch number:6920, Epoch:001, Training: Loss: 0.0017 , Accuracy: 1.0000\n",
      "\"Batch number:6930, Epoch:001, Training: Loss: 0.0004 , Accuracy: 1.0000\n",
      "\"Batch number:6940, Epoch:001, Training: Loss: 0.0011 , Accuracy: 1.0000\n",
      "\"Batch number:6950, Epoch:001, Training: Loss: 0.0041 , Accuracy: 1.0000\n",
      "\"Batch number:6960, Epoch:001, Training: Loss: 0.0017 , Accuracy: 1.0000\n",
      "\"Batch number:6970, Epoch:001, Training: Loss: 0.0060 , Accuracy: 1.0000\n",
      "\"Batch number:6980, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:6990, Epoch:001, Training: Loss: 0.0005 , Accuracy: 1.0000\n",
      "\"Batch number:7000, Epoch:001, Training: Loss: 0.3594 , Accuracy: 0.7500\n",
      "\"Batch number:7010, Epoch:001, Training: Loss: 0.0074 , Accuracy: 1.0000\n",
      "\"Batch number:7020, Epoch:001, Training: Loss: 0.0024 , Accuracy: 1.0000\n",
      "\"Batch number:7030, Epoch:001, Training: Loss: 1.3559 , Accuracy: 0.7500\n",
      "\"Batch number:7040, Epoch:001, Training: Loss: 0.1127 , Accuracy: 1.0000\n",
      "\"Batch number:7050, Epoch:001, Training: Loss: 0.1023 , Accuracy: 1.0000\n",
      "\"Batch number:7060, Epoch:001, Training: Loss: 0.0049 , Accuracy: 1.0000\n",
      "\"Batch number:7070, Epoch:001, Training: Loss: 0.0236 , Accuracy: 1.0000\n",
      "\"Batch number:7080, Epoch:001, Training: Loss: 0.0156 , Accuracy: 1.0000\n",
      "\"Batch number:7090, Epoch:001, Training: Loss: 0.0013 , Accuracy: 1.0000\n",
      "\"Batch number:7100, Epoch:001, Training: Loss: 0.0162 , Accuracy: 1.0000\n",
      "\"Batch number:7110, Epoch:001, Training: Loss: 0.0017 , Accuracy: 1.0000\n",
      "\"Batch number:7120, Epoch:001, Training: Loss: 0.0072 , Accuracy: 1.0000\n",
      "\"Batch number:7130, Epoch:001, Training: Loss: 0.0018 , Accuracy: 1.0000\n",
      "\"Batch number:7140, Epoch:001, Training: Loss: 0.0207 , Accuracy: 1.0000\n",
      "\"Batch number:7150, Epoch:001, Training: Loss: 0.8651 , Accuracy: 0.7500\n",
      "\"Batch number:7160, Epoch:001, Training: Loss: 9.2206 , Accuracy: 0.0000\n",
      "\"Batch number:7170, Epoch:001, Training: Loss: 3.8009 , Accuracy: 0.0000\n",
      "\"Batch number:7180, Epoch:001, Training: Loss: 3.0669 , Accuracy: 0.0000\n",
      "\"Batch number:7190, Epoch:001, Training: Loss: 3.1666 , Accuracy: 0.0000\n",
      "\"Batch number:7200, Epoch:001, Training: Loss: 3.6701 , Accuracy: 0.0000\n",
      "\"Batch number:7210, Epoch:001, Training: Loss: 1.4386 , Accuracy: 0.5000\n",
      "\"Batch number:7220, Epoch:001, Training: Loss: 1.8621 , Accuracy: 0.2500\n",
      "\"Batch number:7230, Epoch:001, Training: Loss: 2.8241 , Accuracy: 0.5000\n",
      "\"Batch number:7240, Epoch:001, Training: Loss: 2.5266 , Accuracy: 0.5000\n",
      "\"Batch number:7250, Epoch:001, Training: Loss: 1.9006 , Accuracy: 0.5000\n",
      "\"Batch number:7260, Epoch:001, Training: Loss: 0.7563 , Accuracy: 1.0000\n",
      "\"Batch number:7270, Epoch:001, Training: Loss: 2.3280 , Accuracy: 0.5000\n",
      "\"Batch number:7280, Epoch:001, Training: Loss: 1.8147 , Accuracy: 0.5000\n",
      "\"Batch number:7290, Epoch:001, Training: Loss: 0.7910 , Accuracy: 1.0000\n",
      "\"Batch number:7300, Epoch:001, Training: Loss: 0.2910 , Accuracy: 1.0000\n",
      "\"Batch number:7310, Epoch:001, Training: Loss: 1.8915 , Accuracy: 0.2500\n",
      "\"Batch number:7320, Epoch:001, Training: Loss: 1.7723 , Accuracy: 0.2500\n",
      "\"Batch number:7330, Epoch:001, Training: Loss: 1.1575 , Accuracy: 0.7500\n",
      "\"Batch number:7340, Epoch:001, Training: Loss: 2.2846 , Accuracy: 0.2500\n",
      "\"Batch number:7350, Epoch:001, Training: Loss: 1.1210 , Accuracy: 0.5000\n",
      "\"Batch number:7360, Epoch:001, Training: Loss: 1.0776 , Accuracy: 0.7500\n",
      "\"Batch number:7370, Epoch:001, Training: Loss: 0.9541 , Accuracy: 0.7500\n",
      "\"Batch number:7380, Epoch:001, Training: Loss: 0.3560 , Accuracy: 1.0000\n",
      "\"Batch number:7390, Epoch:001, Training: Loss: 0.4322 , Accuracy: 1.0000\n",
      "\"Batch number:7400, Epoch:001, Training: Loss: 0.2959 , Accuracy: 1.0000\n",
      "\"Batch number:7410, Epoch:001, Training: Loss: 0.6298 , Accuracy: 0.7500\n",
      "\"Batch number:7420, Epoch:001, Training: Loss: 1.7081 , Accuracy: 0.5000\n",
      "\"Batch number:7430, Epoch:001, Training: Loss: 0.4243 , Accuracy: 1.0000\n",
      "\"Batch number:7440, Epoch:001, Training: Loss: 0.8352 , Accuracy: 0.7500\n",
      "\"Batch number:7450, Epoch:001, Training: Loss: 0.4408 , Accuracy: 1.0000\n",
      "\"Batch number:7460, Epoch:001, Training: Loss: 0.9198 , Accuracy: 0.5000\n",
      "\"Batch number:7470, Epoch:001, Training: Loss: 1.4833 , Accuracy: 0.5000\n",
      "\"Batch number:7480, Epoch:001, Training: Loss: 1.1561 , Accuracy: 0.7500\n",
      "\"Batch number:7490, Epoch:001, Training: Loss: 0.6030 , Accuracy: 0.7500\n",
      "\"Batch number:7500, Epoch:001, Training: Loss: 0.2653 , Accuracy: 1.0000\n",
      "\"Batch number:7510, Epoch:001, Training: Loss: 1.2093 , Accuracy: 0.5000\n",
      "\"Batch number:7520, Epoch:001, Training: Loss: 0.5358 , Accuracy: 0.7500\n",
      "\"Batch number:7530, Epoch:001, Training: Loss: 0.0673 , Accuracy: 1.0000\n",
      "\"Batch number:7540, Epoch:001, Training: Loss: 0.3622 , Accuracy: 1.0000\n",
      "\"Batch number:7550, Epoch:001, Training: Loss: 0.0796 , Accuracy: 1.0000\n",
      "\"Batch number:7560, Epoch:001, Training: Loss: 0.3797 , Accuracy: 1.0000\n",
      "\"Batch number:7570, Epoch:001, Training: Loss: 0.2652 , Accuracy: 1.0000\n",
      "\"Batch number:7580, Epoch:001, Training: Loss: 0.1884 , Accuracy: 1.0000\n",
      "\"Batch number:7590, Epoch:001, Training: Loss: 0.4057 , Accuracy: 1.0000\n",
      "\"Batch number:7600, Epoch:001, Training: Loss: 0.8441 , Accuracy: 0.7500\n",
      "\"Batch number:7610, Epoch:001, Training: Loss: 0.2155 , Accuracy: 1.0000\n",
      "\"Batch number:7620, Epoch:001, Training: Loss: 0.0990 , Accuracy: 1.0000\n",
      "\"Batch number:7630, Epoch:001, Training: Loss: 0.3464 , Accuracy: 1.0000\n",
      "\"Batch number:7640, Epoch:001, Training: Loss: 1.5806 , Accuracy: 0.7500\n",
      "\"Batch number:7650, Epoch:001, Training: Loss: 0.4141 , Accuracy: 0.7500\n",
      "\"Batch number:7660, Epoch:001, Training: Loss: 0.0254 , Accuracy: 1.0000\n",
      "\"Batch number:7670, Epoch:001, Training: Loss: 0.3702 , Accuracy: 1.0000\n",
      "\"Batch number:7680, Epoch:001, Training: Loss: 0.2658 , Accuracy: 1.0000\n",
      "\"Batch number:7690, Epoch:001, Training: Loss: 0.0609 , Accuracy: 1.0000\n",
      "\"Batch number:7700, Epoch:001, Training: Loss: 0.2547 , Accuracy: 1.0000\n",
      "\"Batch number:7710, Epoch:001, Training: Loss: 0.0448 , Accuracy: 1.0000\n",
      "\"Batch number:7720, Epoch:001, Training: Loss: 0.2061 , Accuracy: 1.0000\n",
      "\"Batch number:7730, Epoch:001, Training: Loss: 0.0452 , Accuracy: 1.0000\n",
      "\"Batch number:7740, Epoch:001, Training: Loss: 0.3790 , Accuracy: 0.7500\n",
      "\"Batch number:7750, Epoch:001, Training: Loss: 0.0350 , Accuracy: 1.0000\n",
      "\"Batch number:7760, Epoch:001, Training: Loss: 0.2758 , Accuracy: 1.0000\n",
      "\"Batch number:7770, Epoch:001, Training: Loss: 0.0943 , Accuracy: 1.0000\n",
      "\"Batch number:7780, Epoch:001, Training: Loss: 0.5204 , Accuracy: 1.0000\n",
      "\"Batch number:7790, Epoch:001, Training: Loss: 0.0189 , Accuracy: 1.0000\n",
      "\"Batch number:7800, Epoch:001, Training: Loss: 0.1188 , Accuracy: 1.0000\n",
      "\"Batch number:7810, Epoch:001, Training: Loss: 0.2307 , Accuracy: 1.0000\n",
      "\"Batch number:7820, Epoch:001, Training: Loss: 0.0900 , Accuracy: 1.0000\n",
      "\"Batch number:7830, Epoch:001, Training: Loss: 0.0988 , Accuracy: 1.0000\n",
      "\"Batch number:7840, Epoch:001, Training: Loss: 0.0071 , Accuracy: 1.0000\n",
      "\"Batch number:7850, Epoch:001, Training: Loss: 0.1901 , Accuracy: 1.0000\n",
      "\"Batch number:7860, Epoch:001, Training: Loss: 0.5054 , Accuracy: 0.7500\n",
      "\"Batch number:7870, Epoch:001, Training: Loss: 0.0322 , Accuracy: 1.0000\n",
      "\"Batch number:7880, Epoch:001, Training: Loss: 0.2268 , Accuracy: 1.0000\n",
      "\"Batch number:7890, Epoch:001, Training: Loss: 0.0214 , Accuracy: 1.0000\n",
      "\"Batch number:7900, Epoch:001, Training: Loss: 0.0389 , Accuracy: 1.0000\n",
      "\"Batch number:7910, Epoch:001, Training: Loss: 0.0329 , Accuracy: 1.0000\n",
      "\"Batch number:7920, Epoch:001, Training: Loss: 0.2241 , Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Batch number:7930, Epoch:001, Training: Loss: 0.0560 , Accuracy: 1.0000\n",
      "\"Batch number:7940, Epoch:001, Training: Loss: 0.0826 , Accuracy: 1.0000\n",
      "\"Batch number:7950, Epoch:001, Training: Loss: 0.0419 , Accuracy: 1.0000\n",
      "\"Batch number:7960, Epoch:001, Training: Loss: 0.0652 , Accuracy: 1.0000\n",
      "\"Batch number:7970, Epoch:001, Training: Loss: 0.2059 , Accuracy: 1.0000\n",
      "\"Batch number:7980, Epoch:001, Training: Loss: 0.1663 , Accuracy: 1.0000\n",
      "\"Batch number:7990, Epoch:001, Training: Loss: 0.1848 , Accuracy: 1.0000\n",
      "\"Batch number:8000, Epoch:001, Training: Loss: 0.0344 , Accuracy: 1.0000\n",
      "\"Batch number:8010, Epoch:001, Training: Loss: 0.0147 , Accuracy: 1.0000\n",
      "\"Batch number:8020, Epoch:001, Training: Loss: 0.0091 , Accuracy: 1.0000\n",
      "\"Batch number:8030, Epoch:001, Training: Loss: 0.1168 , Accuracy: 1.0000\n",
      "\"Batch number:8040, Epoch:001, Training: Loss: 0.5649 , Accuracy: 0.7500\n",
      "\"Batch number:8050, Epoch:001, Training: Loss: 0.1769 , Accuracy: 1.0000\n",
      "\"Batch number:8060, Epoch:001, Training: Loss: 0.0807 , Accuracy: 1.0000\n",
      "\"Batch number:8070, Epoch:001, Training: Loss: 0.0104 , Accuracy: 1.0000\n",
      "\"Batch number:8080, Epoch:001, Training: Loss: 0.0360 , Accuracy: 1.0000\n",
      "\"Batch number:8090, Epoch:001, Training: Loss: 0.0745 , Accuracy: 1.0000\n",
      "\"Batch number:8100, Epoch:001, Training: Loss: 0.0486 , Accuracy: 1.0000\n",
      "\"Batch number:8110, Epoch:001, Training: Loss: 0.1183 , Accuracy: 1.0000\n",
      "\"Batch number:8120, Epoch:001, Training: Loss: 0.0268 , Accuracy: 1.0000\n",
      "\"Batch number:8130, Epoch:001, Training: Loss: 0.0167 , Accuracy: 1.0000\n",
      "\"Batch number:8140, Epoch:001, Training: Loss: 0.0202 , Accuracy: 1.0000\n",
      "\"Batch number:8150, Epoch:001, Training: Loss: 0.3189 , Accuracy: 0.7500\n",
      "\"Batch number:8160, Epoch:001, Training: Loss: 0.0367 , Accuracy: 1.0000\n",
      "\"Batch number:8170, Epoch:001, Training: Loss: 0.0167 , Accuracy: 1.0000\n",
      "\"Batch number:8180, Epoch:001, Training: Loss: 0.0109 , Accuracy: 1.0000\n",
      "\"Batch number:8190, Epoch:001, Training: Loss: 0.0312 , Accuracy: 1.0000\n",
      "\"Batch number:8200, Epoch:001, Training: Loss: 0.0083 , Accuracy: 1.0000\n",
      "\"Batch number:8210, Epoch:001, Training: Loss: 0.0161 , Accuracy: 1.0000\n",
      "\"Batch number:8220, Epoch:001, Training: Loss: 0.0089 , Accuracy: 1.0000\n",
      "\"Batch number:8230, Epoch:001, Training: Loss: 0.6832 , Accuracy: 0.7500\n",
      "\"Batch number:8240, Epoch:001, Training: Loss: 0.0184 , Accuracy: 1.0000\n",
      "\"Batch number:8250, Epoch:001, Training: Loss: 0.0465 , Accuracy: 1.0000\n",
      "\"Batch number:8260, Epoch:001, Training: Loss: 0.0033 , Accuracy: 1.0000\n",
      "\"Batch number:8270, Epoch:001, Training: Loss: 0.0072 , Accuracy: 1.0000\n",
      "\"Batch number:8280, Epoch:001, Training: Loss: 0.0270 , Accuracy: 1.0000\n",
      "\"Batch number:8290, Epoch:001, Training: Loss: 0.0961 , Accuracy: 1.0000\n",
      "\"Batch number:8300, Epoch:001, Training: Loss: 0.3155 , Accuracy: 0.7500\n",
      "\"Batch number:8310, Epoch:001, Training: Loss: 0.0295 , Accuracy: 1.0000\n",
      "\"Batch number:8320, Epoch:001, Training: Loss: 0.0182 , Accuracy: 1.0000\n",
      "\"Batch number:8330, Epoch:001, Training: Loss: 0.0436 , Accuracy: 1.0000\n",
      "\"Batch number:8340, Epoch:001, Training: Loss: 0.0135 , Accuracy: 1.0000\n",
      "\"Batch number:8350, Epoch:001, Training: Loss: 0.0344 , Accuracy: 1.0000\n",
      "\"Batch number:8360, Epoch:001, Training: Loss: 0.0335 , Accuracy: 1.0000\n",
      "\"Batch number:8370, Epoch:001, Training: Loss: 0.0019 , Accuracy: 1.0000\n",
      "\"Batch number:8380, Epoch:001, Training: Loss: 0.8706 , Accuracy: 0.7500\n",
      "\"Batch number:8390, Epoch:001, Training: Loss: 0.0177 , Accuracy: 1.0000\n",
      "\"Batch number:8400, Epoch:001, Training: Loss: 0.0064 , Accuracy: 1.0000\n",
      "\"Batch number:8410, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:8420, Epoch:001, Training: Loss: 0.0097 , Accuracy: 1.0000\n",
      "\"Batch number:8430, Epoch:001, Training: Loss: 0.0129 , Accuracy: 1.0000\n",
      "\"Batch number:8440, Epoch:001, Training: Loss: 0.0053 , Accuracy: 1.0000\n",
      "\"Batch number:8450, Epoch:001, Training: Loss: 0.0369 , Accuracy: 1.0000\n",
      "\"Batch number:8460, Epoch:001, Training: Loss: 0.0206 , Accuracy: 1.0000\n",
      "\"Batch number:8470, Epoch:001, Training: Loss: 0.0166 , Accuracy: 1.0000\n",
      "\"Batch number:8480, Epoch:001, Training: Loss: 0.0042 , Accuracy: 1.0000\n",
      "\"Batch number:8490, Epoch:001, Training: Loss: 0.0098 , Accuracy: 1.0000\n",
      "\"Batch number:8500, Epoch:001, Training: Loss: 0.6812 , Accuracy: 0.7500\n",
      "\"Batch number:8510, Epoch:001, Training: Loss: 0.0474 , Accuracy: 1.0000\n",
      "\"Batch number:8520, Epoch:001, Training: Loss: 0.0050 , Accuracy: 1.0000\n",
      "\"Batch number:8530, Epoch:001, Training: Loss: 0.0041 , Accuracy: 1.0000\n",
      "\"Batch number:8540, Epoch:001, Training: Loss: 0.4107 , Accuracy: 0.7500\n",
      "\"Batch number:8550, Epoch:001, Training: Loss: 0.0783 , Accuracy: 1.0000\n",
      "\"Batch number:8560, Epoch:001, Training: Loss: 0.0108 , Accuracy: 1.0000\n",
      "\"Batch number:8570, Epoch:001, Training: Loss: 0.0229 , Accuracy: 1.0000\n",
      "\"Batch number:8580, Epoch:001, Training: Loss: 0.0011 , Accuracy: 1.0000\n",
      "\"Batch number:8590, Epoch:001, Training: Loss: 0.0763 , Accuracy: 1.0000\n",
      "\"Batch number:8600, Epoch:001, Training: Loss: 0.0862 , Accuracy: 1.0000\n",
      "\"Batch number:8610, Epoch:001, Training: Loss: 0.0220 , Accuracy: 1.0000\n",
      "\"Batch number:8620, Epoch:001, Training: Loss: 0.0041 , Accuracy: 1.0000\n",
      "\"Batch number:8630, Epoch:001, Training: Loss: 0.3384 , Accuracy: 1.0000\n",
      "\"Batch number:8640, Epoch:001, Training: Loss: 0.0015 , Accuracy: 1.0000\n",
      "\"Batch number:8650, Epoch:001, Training: Loss: 0.0079 , Accuracy: 1.0000\n",
      "\"Batch number:8660, Epoch:001, Training: Loss: 0.0022 , Accuracy: 1.0000\n",
      "\"Batch number:8670, Epoch:001, Training: Loss: 0.0163 , Accuracy: 1.0000\n",
      "\"Batch number:8680, Epoch:001, Training: Loss: 0.0276 , Accuracy: 1.0000\n",
      "\"Batch number:8690, Epoch:001, Training: Loss: 0.0015 , Accuracy: 1.0000\n",
      "\"Batch number:8700, Epoch:001, Training: Loss: 0.0858 , Accuracy: 1.0000\n",
      "\"Batch number:8710, Epoch:001, Training: Loss: 0.0029 , Accuracy: 1.0000\n",
      "\"Batch number:8720, Epoch:001, Training: Loss: 0.0057 , Accuracy: 1.0000\n",
      "\"Batch number:8730, Epoch:001, Training: Loss: 0.0084 , Accuracy: 1.0000\n",
      "\"Batch number:8740, Epoch:001, Training: Loss: 0.0116 , Accuracy: 1.0000\n",
      "\"Batch number:8750, Epoch:001, Training: Loss: 0.0008 , Accuracy: 1.0000\n",
      "\"Batch number:8760, Epoch:001, Training: Loss: 0.0074 , Accuracy: 1.0000\n",
      "\"Batch number:8770, Epoch:001, Training: Loss: 0.0116 , Accuracy: 1.0000\n",
      "\"Batch number:8780, Epoch:001, Training: Loss: 0.0044 , Accuracy: 1.0000\n",
      "\"Batch number:8790, Epoch:001, Training: Loss: 0.0072 , Accuracy: 1.0000\n",
      "\"Batch number:8800, Epoch:001, Training: Loss: 0.0033 , Accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute the total loss for the batch and add it to train_loss\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute the accuracy\u001b[39;00m\n\u001b[1;32m     24\u001b[0m ret, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train_loss =0.0\n",
    "    train_acc = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        #print(i,data)\n",
    "        images,labels = data['images'],data['labels']\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        ## zero the parameter gradients # Clean existing gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## forward + backward + optimize\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = net(images)\n",
    "        # Compute loss\n",
    "        loss = loss_criterion(outputs,labels.reshape(-1))\n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        # Compute the total loss for the batch and add it to train_loss\n",
    "        train_loss +=loss.item() * images.size(0)\n",
    "        # Compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        # Compute total accuracy in the whole batch and add to train_acc\n",
    "        train_acc += acc.item() * images.size(0)\n",
    "        if i%10 == 9: ## print every 100 mini batches\n",
    "            print('\"Batch number:{:03d}, Epoch:{:03d}, Training: Loss: {:.4f} , Accuracy: {:.4f}'.format(i+1,epoch+1, loss.item(), acc.item()))\n",
    "                # epoch + 1, i + 1, running_loss / 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc14f62",
   "metadata": {},
   "source": [
    "validation loop:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9d8b53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 000, Validation: Loss: 10.0947, Accuracy: 0.0000\n",
      "Validation Batch number: 001, Validation: Loss: 11.9529, Accuracy: 0.2500\n",
      "Validation Batch number: 002, Validation: Loss: 11.1037, Accuracy: 0.2500\n",
      "Validation Batch number: 003, Validation: Loss: 8.5319, Accuracy: 0.2500\n",
      "Validation Batch number: 004, Validation: Loss: 11.1411, Accuracy: 0.0000\n",
      "Validation Batch number: 005, Validation: Loss: 9.4625, Accuracy: 0.0000\n",
      "Validation Batch number: 006, Validation: Loss: 4.6716, Accuracy: 0.5000\n",
      "Validation Batch number: 007, Validation: Loss: 11.0463, Accuracy: 0.0000\n",
      "Validation Batch number: 008, Validation: Loss: 5.4153, Accuracy: 0.2500\n",
      "Validation Batch number: 009, Validation: Loss: 8.1421, Accuracy: 0.2500\n",
      "Validation Batch number: 010, Validation: Loss: 7.7571, Accuracy: 0.0000\n",
      "Validation Batch number: 011, Validation: Loss: 6.0851, Accuracy: 0.2500\n",
      "Validation Batch number: 012, Validation: Loss: 8.0812, Accuracy: 0.0000\n",
      "Validation Batch number: 013, Validation: Loss: 5.8630, Accuracy: 0.2500\n",
      "Validation Batch number: 014, Validation: Loss: 5.2674, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 9.7921, Accuracy: 0.2500\n",
      "Validation Batch number: 016, Validation: Loss: 7.1006, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 8.7993, Accuracy: 0.0000\n",
      "Validation Batch number: 018, Validation: Loss: 7.8243, Accuracy: 0.0000\n",
      "Validation Batch number: 019, Validation: Loss: 8.2632, Accuracy: 0.2500\n",
      "Validation Batch number: 020, Validation: Loss: 4.2886, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 4.5320, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 7.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 7.6526, Accuracy: 0.2500\n",
      "Validation Batch number: 024, Validation: Loss: 3.5870, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 5.4206, Accuracy: 0.2500\n",
      "Validation Batch number: 026, Validation: Loss: 9.3868, Accuracy: 0.0000\n",
      "Validation Batch number: 027, Validation: Loss: 10.7490, Accuracy: 0.0000\n",
      "Validation Batch number: 028, Validation: Loss: 8.4802, Accuracy: 0.2500\n",
      "Validation Batch number: 029, Validation: Loss: 4.3750, Accuracy: 0.5000\n",
      "Validation Batch number: 030, Validation: Loss: 8.1153, Accuracy: 0.2500\n",
      "Validation Batch number: 031, Validation: Loss: 6.2492, Accuracy: 0.2500\n",
      "Validation Batch number: 032, Validation: Loss: 11.6586, Accuracy: 0.0000\n",
      "Validation Batch number: 033, Validation: Loss: 9.4649, Accuracy: 0.0000\n",
      "Validation Batch number: 034, Validation: Loss: 5.0257, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 7.5500, Accuracy: 0.5000\n",
      "Validation Batch number: 036, Validation: Loss: 4.8380, Accuracy: 0.5000\n",
      "Validation Batch number: 037, Validation: Loss: 11.2387, Accuracy: 0.0000\n",
      "Validation Batch number: 038, Validation: Loss: 8.1418, Accuracy: 0.2500\n",
      "Validation Batch number: 039, Validation: Loss: 9.7553, Accuracy: 0.2500\n",
      "Validation Batch number: 040, Validation: Loss: 7.7965, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 8.9288, Accuracy: 0.2500\n",
      "Validation Batch number: 042, Validation: Loss: 9.9818, Accuracy: 0.0000\n",
      "Validation Batch number: 043, Validation: Loss: 6.4718, Accuracy: 0.2500\n",
      "Validation Batch number: 044, Validation: Loss: 11.4452, Accuracy: 0.0000\n",
      "Validation Batch number: 045, Validation: Loss: 13.1098, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 4.2503, Accuracy: 0.5000\n",
      "Validation Batch number: 047, Validation: Loss: 1.8925, Accuracy: 0.5000\n",
      "Validation Batch number: 048, Validation: Loss: 6.9568, Accuracy: 0.2500\n",
      "Validation Batch number: 049, Validation: Loss: 14.9849, Accuracy: 0.0000\n",
      "Validation Batch number: 050, Validation: Loss: 11.7108, Accuracy: 0.0000\n",
      "Validation Batch number: 051, Validation: Loss: 8.7360, Accuracy: 0.2500\n",
      "Validation Batch number: 052, Validation: Loss: 5.0464, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 7.5208, Accuracy: 0.0000\n",
      "Validation Batch number: 054, Validation: Loss: 7.0850, Accuracy: 0.0000\n",
      "Validation Batch number: 055, Validation: Loss: 10.9875, Accuracy: 0.0000\n",
      "Validation Batch number: 056, Validation: Loss: 8.4362, Accuracy: 0.2500\n",
      "Validation Batch number: 057, Validation: Loss: 7.2364, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 6.6576, Accuracy: 0.2500\n",
      "Validation Batch number: 059, Validation: Loss: 8.2239, Accuracy: 0.2500\n",
      "Validation Batch number: 060, Validation: Loss: 11.6169, Accuracy: 0.0000\n",
      "Validation Batch number: 061, Validation: Loss: 5.0702, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 7.6703, Accuracy: 0.0000\n",
      "Validation Batch number: 063, Validation: Loss: 5.7983, Accuracy: 0.2500\n",
      "Validation Batch number: 064, Validation: Loss: 7.7669, Accuracy: 0.0000\n",
      "Validation Batch number: 065, Validation: Loss: 6.0482, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 6.2138, Accuracy: 0.0000\n",
      "Validation Batch number: 067, Validation: Loss: 7.0923, Accuracy: 0.2500\n",
      "Validation Batch number: 068, Validation: Loss: 9.1213, Accuracy: 0.2500\n",
      "Validation Batch number: 069, Validation: Loss: 6.9495, Accuracy: 0.2500\n",
      "Validation Batch number: 070, Validation: Loss: 3.6223, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 10.7384, Accuracy: 0.0000\n",
      "Validation Batch number: 072, Validation: Loss: 12.8083, Accuracy: 0.0000\n",
      "Validation Batch number: 073, Validation: Loss: 3.7936, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 6.2638, Accuracy: 0.2500\n",
      "Validation Batch number: 075, Validation: Loss: 5.3303, Accuracy: 0.2500\n",
      "Validation Batch number: 076, Validation: Loss: 6.7684, Accuracy: 0.2500\n",
      "Validation Batch number: 077, Validation: Loss: 8.5515, Accuracy: 0.2500\n",
      "Validation Batch number: 078, Validation: Loss: 7.9476, Accuracy: 0.5000\n",
      "Validation Batch number: 079, Validation: Loss: 6.5858, Accuracy: 0.2500\n",
      "Validation Batch number: 080, Validation: Loss: 6.5852, Accuracy: 0.2500\n",
      "Validation Batch number: 081, Validation: Loss: 6.3432, Accuracy: 0.0000\n",
      "Validation Batch number: 082, Validation: Loss: 10.4810, Accuracy: 0.0000\n",
      "Validation Batch number: 083, Validation: Loss: 2.3158, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 8.3332, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 6.0061, Accuracy: 0.2500\n",
      "Validation Batch number: 086, Validation: Loss: 6.2760, Accuracy: 0.2500\n",
      "Validation Batch number: 087, Validation: Loss: 5.7815, Accuracy: 0.2500\n",
      "Validation Batch number: 088, Validation: Loss: 9.4516, Accuracy: 0.2500\n",
      "Validation Batch number: 089, Validation: Loss: 8.7496, Accuracy: 0.0000\n",
      "Validation Batch number: 090, Validation: Loss: 9.3107, Accuracy: 0.2500\n",
      "Validation Batch number: 091, Validation: Loss: 8.4777, Accuracy: 0.0000\n",
      "Validation Batch number: 092, Validation: Loss: 5.1135, Accuracy: 0.5000\n",
      "Validation Batch number: 093, Validation: Loss: 4.9019, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 4.4239, Accuracy: 0.2500\n",
      "Validation Batch number: 095, Validation: Loss: 10.9705, Accuracy: 0.0000\n",
      "Validation Batch number: 096, Validation: Loss: 6.7721, Accuracy: 0.2500\n",
      "Validation Batch number: 097, Validation: Loss: 6.4300, Accuracy: 0.0000\n",
      "Validation Batch number: 098, Validation: Loss: 4.5790, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 13.8897, Accuracy: 0.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5908, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 4.3996, Accuracy: 0.5000\n",
      "Validation Batch number: 102, Validation: Loss: 6.0354, Accuracy: 0.2500\n",
      "Validation Batch number: 103, Validation: Loss: 8.3635, Accuracy: 0.0000\n",
      "Validation Batch number: 104, Validation: Loss: 7.3699, Accuracy: 0.2500\n",
      "Validation Batch number: 105, Validation: Loss: 8.0770, Accuracy: 0.0000\n",
      "Validation Batch number: 106, Validation: Loss: 9.2156, Accuracy: 0.2500\n",
      "Validation Batch number: 107, Validation: Loss: 6.2240, Accuracy: 0.2500\n",
      "Validation Batch number: 108, Validation: Loss: 6.7031, Accuracy: 0.0000\n",
      "Validation Batch number: 109, Validation: Loss: 3.2000, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 4.2469, Accuracy: 0.5000\n",
      "Validation Batch number: 111, Validation: Loss: 7.1764, Accuracy: 0.2500\n",
      "Validation Batch number: 112, Validation: Loss: 4.4969, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 7.1897, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 114, Validation: Loss: 5.0639, Accuracy: 0.2500\n",
      "Validation Batch number: 115, Validation: Loss: 8.1621, Accuracy: 0.5000\n",
      "Validation Batch number: 116, Validation: Loss: 5.1070, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 2.9936, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 12.0686, Accuracy: 0.0000\n",
      "Validation Batch number: 119, Validation: Loss: 3.9786, Accuracy: 0.5000\n",
      "Validation Batch number: 120, Validation: Loss: 6.2568, Accuracy: 0.5000\n",
      "Validation Batch number: 121, Validation: Loss: 3.9416, Accuracy: 0.2500\n",
      "Validation Batch number: 122, Validation: Loss: 9.3498, Accuracy: 0.0000\n",
      "Validation Batch number: 123, Validation: Loss: 4.4089, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 2.6730, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 4.4229, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 9.2107, Accuracy: 0.0000\n",
      "Validation Batch number: 127, Validation: Loss: 9.5640, Accuracy: 0.2500\n",
      "Validation Batch number: 128, Validation: Loss: 6.8695, Accuracy: 0.2500\n",
      "Validation Batch number: 129, Validation: Loss: 7.6043, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 9.6738, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 11.8761, Accuracy: 0.0000\n",
      "Validation Batch number: 132, Validation: Loss: 8.9297, Accuracy: 0.2500\n",
      "Validation Batch number: 133, Validation: Loss: 1.6763, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 7.1815, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 8.3065, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 7.0960, Accuracy: 0.0000\n",
      "Validation Batch number: 137, Validation: Loss: 9.8248, Accuracy: 0.2500\n",
      "Validation Batch number: 138, Validation: Loss: 8.1353, Accuracy: 0.2500\n",
      "Validation Batch number: 139, Validation: Loss: 11.9750, Accuracy: 0.0000\n",
      "Validation Batch number: 140, Validation: Loss: 4.0055, Accuracy: 0.5000\n",
      "Validation Batch number: 141, Validation: Loss: 7.1058, Accuracy: 0.0000\n",
      "Validation Batch number: 142, Validation: Loss: 9.1082, Accuracy: 0.2500\n",
      "Validation Batch number: 143, Validation: Loss: 2.6630, Accuracy: 0.7500\n",
      "Validation Batch number: 144, Validation: Loss: 11.6011, Accuracy: 0.0000\n",
      "Validation Batch number: 145, Validation: Loss: 5.2345, Accuracy: 0.5000\n",
      "Validation Batch number: 146, Validation: Loss: 3.9791, Accuracy: 0.5000\n",
      "Validation Batch number: 147, Validation: Loss: 11.7531, Accuracy: 0.0000\n",
      "Validation Batch number: 148, Validation: Loss: 6.1248, Accuracy: 0.2500\n",
      "Validation Batch number: 149, Validation: Loss: 12.1663, Accuracy: 0.0000\n",
      "Validation Batch number: 150, Validation: Loss: 5.6905, Accuracy: 0.2500\n",
      "Validation Batch number: 151, Validation: Loss: 8.4688, Accuracy: 0.0000\n",
      "Validation Batch number: 152, Validation: Loss: 8.9865, Accuracy: 0.2500\n",
      "Validation Batch number: 153, Validation: Loss: 3.7537, Accuracy: 0.5000\n",
      "Validation Batch number: 154, Validation: Loss: 8.2667, Accuracy: 0.0000\n",
      "Validation Batch number: 155, Validation: Loss: 4.3273, Accuracy: 0.5000\n",
      "Validation Batch number: 156, Validation: Loss: 6.4519, Accuracy: 0.5000\n",
      "Validation Batch number: 157, Validation: Loss: 4.0656, Accuracy: 0.5000\n",
      "Validation Batch number: 158, Validation: Loss: 7.3986, Accuracy: 0.2500\n",
      "Validation Batch number: 159, Validation: Loss: 6.5289, Accuracy: 0.2500\n",
      "Validation Batch number: 160, Validation: Loss: 9.5699, Accuracy: 0.2500\n",
      "Validation Batch number: 161, Validation: Loss: 13.0097, Accuracy: 0.0000\n",
      "Validation Batch number: 162, Validation: Loss: 12.1523, Accuracy: 0.0000\n",
      "Validation Batch number: 163, Validation: Loss: 7.5986, Accuracy: 0.2500\n",
      "Validation Batch number: 164, Validation: Loss: 9.5627, Accuracy: 0.0000\n",
      "Validation Batch number: 165, Validation: Loss: 7.9666, Accuracy: 0.2500\n",
      "Validation Batch number: 166, Validation: Loss: 7.5048, Accuracy: 0.2500\n",
      "Validation Batch number: 167, Validation: Loss: 4.1017, Accuracy: 0.5000\n",
      "Validation Batch number: 168, Validation: Loss: 5.6905, Accuracy: 0.2500\n",
      "Validation Batch number: 169, Validation: Loss: 7.2543, Accuracy: 0.0000\n",
      "Validation Batch number: 170, Validation: Loss: 4.3374, Accuracy: 0.5000\n",
      "Validation Batch number: 171, Validation: Loss: 12.6030, Accuracy: 0.2500\n",
      "Validation Batch number: 172, Validation: Loss: 9.9206, Accuracy: 0.0000\n",
      "Validation Batch number: 173, Validation: Loss: 3.1480, Accuracy: 0.5000\n",
      "Validation Batch number: 174, Validation: Loss: 9.1717, Accuracy: 0.2500\n",
      "Validation Batch number: 175, Validation: Loss: 8.7849, Accuracy: 0.2500\n",
      "Validation Batch number: 176, Validation: Loss: 6.0074, Accuracy: 0.2500\n",
      "Validation Batch number: 177, Validation: Loss: 8.0456, Accuracy: 0.0000\n",
      "Validation Batch number: 178, Validation: Loss: 6.9141, Accuracy: 0.2500\n",
      "Validation Batch number: 179, Validation: Loss: 9.6391, Accuracy: 0.0000\n",
      "Validation Batch number: 180, Validation: Loss: 4.5980, Accuracy: 0.2500\n",
      "Validation Batch number: 181, Validation: Loss: 7.5116, Accuracy: 0.2500\n",
      "Validation Batch number: 182, Validation: Loss: 4.1753, Accuracy: 0.5000\n",
      "Validation Batch number: 183, Validation: Loss: 8.9649, Accuracy: 0.2500\n",
      "Validation Batch number: 184, Validation: Loss: 6.0988, Accuracy: 0.5000\n",
      "Validation Batch number: 185, Validation: Loss: 10.8659, Accuracy: 0.2500\n",
      "Validation Batch number: 186, Validation: Loss: 9.2422, Accuracy: 0.2500\n",
      "Validation Batch number: 187, Validation: Loss: 9.5706, Accuracy: 0.2500\n",
      "Validation Batch number: 188, Validation: Loss: 6.4555, Accuracy: 0.5000\n",
      "Validation Batch number: 189, Validation: Loss: 5.8018, Accuracy: 0.2500\n",
      "Validation Batch number: 190, Validation: Loss: 10.6384, Accuracy: 0.2500\n",
      "Validation Batch number: 191, Validation: Loss: 7.6027, Accuracy: 0.2500\n",
      "Validation Batch number: 192, Validation: Loss: 15.2349, Accuracy: 0.0000\n",
      "Validation Batch number: 193, Validation: Loss: 10.1160, Accuracy: 0.2500\n",
      "Validation Batch number: 194, Validation: Loss: 8.4732, Accuracy: 0.2500\n",
      "Validation Batch number: 195, Validation: Loss: 5.4210, Accuracy: 0.5000\n",
      "Validation Batch number: 196, Validation: Loss: 7.0732, Accuracy: 0.2500\n",
      "Validation Batch number: 197, Validation: Loss: 5.8180, Accuracy: 0.5000\n",
      "Validation Batch number: 198, Validation: Loss: 5.4077, Accuracy: 0.5000\n",
      "Validation Batch number: 199, Validation: Loss: 11.0230, Accuracy: 0.0000\n",
      "Validation Batch number: 200, Validation: Loss: 9.2925, Accuracy: 0.2500\n",
      "Validation Batch number: 201, Validation: Loss: 5.1616, Accuracy: 0.2500\n",
      "Validation Batch number: 202, Validation: Loss: 6.7974, Accuracy: 0.2500\n",
      "Validation Batch number: 203, Validation: Loss: 11.6804, Accuracy: 0.2500\n",
      "Validation Batch number: 204, Validation: Loss: 4.7610, Accuracy: 0.2500\n",
      "Validation Batch number: 205, Validation: Loss: 1.6520, Accuracy: 0.7500\n",
      "Validation Batch number: 206, Validation: Loss: 7.7929, Accuracy: 0.2500\n",
      "Validation Batch number: 207, Validation: Loss: 7.9403, Accuracy: 0.2500\n",
      "Validation Batch number: 208, Validation: Loss: 3.3945, Accuracy: 0.5000\n",
      "Validation Batch number: 209, Validation: Loss: 10.6717, Accuracy: 0.2500\n",
      "Validation Batch number: 210, Validation: Loss: 4.8158, Accuracy: 0.2500\n",
      "Validation Batch number: 211, Validation: Loss: 3.2925, Accuracy: 0.5000\n",
      "Validation Batch number: 212, Validation: Loss: 12.7095, Accuracy: 0.0000\n",
      "Validation Batch number: 213, Validation: Loss: 4.9562, Accuracy: 0.5000\n",
      "Validation Batch number: 214, Validation: Loss: 1.7374, Accuracy: 0.7500\n",
      "Validation Batch number: 215, Validation: Loss: 5.0520, Accuracy: 0.2500\n",
      "Validation Batch number: 216, Validation: Loss: 12.1235, Accuracy: 0.0000\n",
      "Validation Batch number: 217, Validation: Loss: 5.7833, Accuracy: 0.5000\n",
      "Validation Batch number: 218, Validation: Loss: 5.0726, Accuracy: 0.5000\n",
      "Validation Batch number: 219, Validation: Loss: 4.0813, Accuracy: 0.5000\n",
      "Validation Batch number: 220, Validation: Loss: 7.8714, Accuracy: 0.2500\n",
      "Validation Batch number: 221, Validation: Loss: 7.8837, Accuracy: 0.2500\n",
      "Validation Batch number: 222, Validation: Loss: 7.1810, Accuracy: 0.2500\n",
      "Validation Batch number: 223, Validation: Loss: 5.0442, Accuracy: 0.5000\n",
      "Validation Batch number: 224, Validation: Loss: 7.9156, Accuracy: 0.2500\n",
      "Validation Batch number: 225, Validation: Loss: 4.2795, Accuracy: 0.5000\n",
      "Validation Batch number: 226, Validation: Loss: 8.0530, Accuracy: 0.0000\n",
      "Validation Batch number: 227, Validation: Loss: 4.7271, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 228, Validation: Loss: 11.5329, Accuracy: 0.0000\n",
      "Validation Batch number: 229, Validation: Loss: 8.9635, Accuracy: 0.0000\n",
      "Validation Batch number: 230, Validation: Loss: 5.5811, Accuracy: 0.2500\n",
      "Validation Batch number: 231, Validation: Loss: 9.5591, Accuracy: 0.0000\n",
      "Validation Batch number: 232, Validation: Loss: 11.8631, Accuracy: 0.0000\n",
      "Validation Batch number: 233, Validation: Loss: 5.3750, Accuracy: 0.5000\n",
      "Validation Batch number: 234, Validation: Loss: 9.4971, Accuracy: 0.2500\n",
      "Validation Batch number: 235, Validation: Loss: 8.7584, Accuracy: 0.2500\n",
      "Validation Batch number: 236, Validation: Loss: 6.5325, Accuracy: 0.2500\n",
      "Validation Batch number: 237, Validation: Loss: 7.7365, Accuracy: 0.2500\n",
      "Validation Batch number: 238, Validation: Loss: 9.3088, Accuracy: 0.0000\n",
      "Validation Batch number: 239, Validation: Loss: 2.8828, Accuracy: 0.5000\n",
      "Validation Batch number: 240, Validation: Loss: 10.8837, Accuracy: 0.2500\n",
      "Validation Batch number: 241, Validation: Loss: 8.3077, Accuracy: 0.2500\n",
      "Validation Batch number: 242, Validation: Loss: 8.0922, Accuracy: 0.2500\n",
      "Validation Batch number: 243, Validation: Loss: 5.4881, Accuracy: 0.0000\n",
      "Validation Batch number: 244, Validation: Loss: 8.7981, Accuracy: 0.2500\n",
      "Validation Batch number: 245, Validation: Loss: 10.5539, Accuracy: 0.0000\n",
      "Validation Batch number: 246, Validation: Loss: 1.3651, Accuracy: 0.7500\n",
      "Validation Batch number: 247, Validation: Loss: 10.9309, Accuracy: 0.2500\n",
      "Validation Batch number: 248, Validation: Loss: 6.9264, Accuracy: 0.2500\n",
      "Validation Batch number: 249, Validation: Loss: 4.3239, Accuracy: 0.5000\n",
      "Validation Batch number: 250, Validation: Loss: 6.9287, Accuracy: 0.0000\n",
      "Validation Batch number: 251, Validation: Loss: 11.8739, Accuracy: 0.2500\n",
      "Validation Batch number: 252, Validation: Loss: 5.1852, Accuracy: 0.2500\n",
      "Validation Batch number: 253, Validation: Loss: 7.4777, Accuracy: 0.2500\n",
      "Validation Batch number: 254, Validation: Loss: 6.0174, Accuracy: 0.2500\n",
      "Validation Batch number: 255, Validation: Loss: 3.4506, Accuracy: 0.5000\n",
      "Validation Batch number: 256, Validation: Loss: 10.4753, Accuracy: 0.0000\n",
      "Validation Batch number: 257, Validation: Loss: 5.1457, Accuracy: 0.5000\n",
      "Validation Batch number: 258, Validation: Loss: 6.7340, Accuracy: 0.2500\n",
      "Validation Batch number: 259, Validation: Loss: 5.0530, Accuracy: 0.2500\n",
      "Validation Batch number: 260, Validation: Loss: 10.1366, Accuracy: 0.0000\n",
      "Validation Batch number: 261, Validation: Loss: 5.5290, Accuracy: 0.2500\n",
      "Validation Batch number: 262, Validation: Loss: 4.6315, Accuracy: 0.5000\n",
      "Validation Batch number: 263, Validation: Loss: 8.3416, Accuracy: 0.2500\n",
      "Validation Batch number: 264, Validation: Loss: 8.7040, Accuracy: 0.2500\n",
      "Validation Batch number: 265, Validation: Loss: 6.1959, Accuracy: 0.5000\n",
      "Validation Batch number: 266, Validation: Loss: 7.0048, Accuracy: 0.2500\n",
      "Validation Batch number: 267, Validation: Loss: 10.9376, Accuracy: 0.0000\n",
      "Validation Batch number: 268, Validation: Loss: 9.0730, Accuracy: 0.0000\n",
      "Validation Batch number: 269, Validation: Loss: 5.1748, Accuracy: 0.5000\n",
      "Validation Batch number: 270, Validation: Loss: 5.6424, Accuracy: 0.5000\n",
      "Validation Batch number: 271, Validation: Loss: 5.0061, Accuracy: 0.5000\n",
      "Validation Batch number: 272, Validation: Loss: 7.8481, Accuracy: 0.2500\n",
      "Validation Batch number: 273, Validation: Loss: 10.3369, Accuracy: 0.0000\n",
      "Validation Batch number: 274, Validation: Loss: 3.5278, Accuracy: 0.7500\n",
      "Validation Batch number: 275, Validation: Loss: 7.9309, Accuracy: 0.2500\n",
      "Validation Batch number: 276, Validation: Loss: 10.7388, Accuracy: 0.0000\n",
      "Validation Batch number: 277, Validation: Loss: 2.9927, Accuracy: 0.5000\n",
      "Validation Batch number: 278, Validation: Loss: 8.0964, Accuracy: 0.2500\n",
      "Validation Batch number: 279, Validation: Loss: 5.8611, Accuracy: 0.2500\n",
      "Validation Batch number: 280, Validation: Loss: 5.6320, Accuracy: 0.2500\n",
      "Validation Batch number: 281, Validation: Loss: 8.5768, Accuracy: 0.0000\n",
      "Validation Batch number: 282, Validation: Loss: 8.1523, Accuracy: 0.2500\n",
      "Validation Batch number: 283, Validation: Loss: 4.0671, Accuracy: 0.5000\n",
      "Validation Batch number: 284, Validation: Loss: 4.9754, Accuracy: 0.5000\n",
      "Validation Batch number: 285, Validation: Loss: 7.7392, Accuracy: 0.2500\n",
      "Validation Batch number: 286, Validation: Loss: 10.0491, Accuracy: 0.2500\n",
      "Validation Batch number: 287, Validation: Loss: 8.0547, Accuracy: 0.2500\n",
      "Validation Batch number: 288, Validation: Loss: 6.6004, Accuracy: 0.2500\n",
      "Validation Batch number: 289, Validation: Loss: 11.5009, Accuracy: 0.2500\n",
      "Validation Batch number: 290, Validation: Loss: 8.3194, Accuracy: 0.0000\n",
      "Validation Batch number: 291, Validation: Loss: 5.3009, Accuracy: 0.5000\n",
      "Validation Batch number: 292, Validation: Loss: 7.9350, Accuracy: 0.2500\n",
      "Validation Batch number: 293, Validation: Loss: 9.5954, Accuracy: 0.0000\n",
      "Validation Batch number: 294, Validation: Loss: 6.3278, Accuracy: 0.2500\n",
      "Validation Batch number: 295, Validation: Loss: 3.6448, Accuracy: 0.2500\n",
      "Validation Batch number: 296, Validation: Loss: 4.8125, Accuracy: 0.0000\n",
      "Validation Batch number: 297, Validation: Loss: 6.8934, Accuracy: 0.5000\n",
      "Validation Batch number: 298, Validation: Loss: 4.4160, Accuracy: 0.5000\n",
      "Validation Batch number: 299, Validation: Loss: 11.0941, Accuracy: 0.0000\n",
      "Validation Batch number: 300, Validation: Loss: 8.4549, Accuracy: 0.0000\n",
      "Validation Batch number: 301, Validation: Loss: 11.2400, Accuracy: 0.0000\n",
      "Validation Batch number: 302, Validation: Loss: 7.9157, Accuracy: 0.5000\n",
      "Validation Batch number: 303, Validation: Loss: 9.1348, Accuracy: 0.2500\n",
      "Validation Batch number: 304, Validation: Loss: 6.7129, Accuracy: 0.2500\n",
      "Validation Batch number: 305, Validation: Loss: 9.2339, Accuracy: 0.2500\n",
      "Validation Batch number: 306, Validation: Loss: 4.6473, Accuracy: 0.2500\n",
      "Validation Batch number: 307, Validation: Loss: 7.3126, Accuracy: 0.0000\n",
      "Validation Batch number: 308, Validation: Loss: 6.8101, Accuracy: 0.2500\n",
      "Validation Batch number: 309, Validation: Loss: 9.6337, Accuracy: 0.0000\n",
      "Validation Batch number: 310, Validation: Loss: 10.2321, Accuracy: 0.0000\n",
      "Validation Batch number: 311, Validation: Loss: 7.0670, Accuracy: 0.2500\n",
      "Validation Batch number: 312, Validation: Loss: 4.5192, Accuracy: 0.7500\n",
      "Validation Batch number: 313, Validation: Loss: 6.2865, Accuracy: 0.2500\n",
      "Validation Batch number: 314, Validation: Loss: 5.4693, Accuracy: 0.5000\n",
      "Validation Batch number: 315, Validation: Loss: 5.1235, Accuracy: 0.5000\n",
      "Validation Batch number: 316, Validation: Loss: 6.5431, Accuracy: 0.2500\n",
      "Validation Batch number: 317, Validation: Loss: 5.3484, Accuracy: 0.2500\n",
      "Validation Batch number: 318, Validation: Loss: 6.4392, Accuracy: 0.2500\n",
      "Validation Batch number: 319, Validation: Loss: 4.0099, Accuracy: 0.2500\n",
      "Validation Batch number: 320, Validation: Loss: 10.7652, Accuracy: 0.0000\n",
      "Validation Batch number: 321, Validation: Loss: 6.5663, Accuracy: 0.2500\n",
      "Validation Batch number: 322, Validation: Loss: 5.4848, Accuracy: 0.2500\n",
      "Validation Batch number: 323, Validation: Loss: 5.9268, Accuracy: 0.2500\n",
      "Validation Batch number: 324, Validation: Loss: 3.0834, Accuracy: 0.5000\n",
      "Validation Batch number: 325, Validation: Loss: 7.7068, Accuracy: 0.2500\n",
      "Validation Batch number: 326, Validation: Loss: 8.7051, Accuracy: 0.2500\n",
      "Validation Batch number: 327, Validation: Loss: 2.3559, Accuracy: 0.7500\n",
      "Validation Batch number: 328, Validation: Loss: 5.4068, Accuracy: 0.5000\n",
      "Validation Batch number: 329, Validation: Loss: 8.0198, Accuracy: 0.2500\n",
      "Validation Batch number: 330, Validation: Loss: 12.7193, Accuracy: 0.2500\n",
      "Validation Batch number: 331, Validation: Loss: 7.3062, Accuracy: 0.5000\n",
      "Validation Batch number: 332, Validation: Loss: 10.5279, Accuracy: 0.2500\n",
      "Validation Batch number: 333, Validation: Loss: 4.5390, Accuracy: 0.5000\n",
      "Validation Batch number: 334, Validation: Loss: 9.7559, Accuracy: 0.2500\n",
      "Validation Batch number: 335, Validation: Loss: 4.7267, Accuracy: 0.5000\n",
      "Validation Batch number: 336, Validation: Loss: 8.9852, Accuracy: 0.0000\n",
      "Validation Batch number: 337, Validation: Loss: 6.6283, Accuracy: 0.2500\n",
      "Validation Batch number: 338, Validation: Loss: 3.8318, Accuracy: 0.5000\n",
      "Validation Batch number: 339, Validation: Loss: 2.0651, Accuracy: 0.7500\n",
      "Validation Batch number: 340, Validation: Loss: 9.5861, Accuracy: 0.0000\n",
      "Validation Batch number: 341, Validation: Loss: 7.8451, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 342, Validation: Loss: 8.4929, Accuracy: 0.5000\n",
      "Validation Batch number: 343, Validation: Loss: 6.8812, Accuracy: 0.2500\n",
      "Validation Batch number: 344, Validation: Loss: 6.3749, Accuracy: 0.2500\n",
      "Validation Batch number: 345, Validation: Loss: 9.5189, Accuracy: 0.0000\n",
      "Validation Batch number: 346, Validation: Loss: 4.2713, Accuracy: 0.5000\n",
      "Validation Batch number: 347, Validation: Loss: 5.3214, Accuracy: 0.5000\n",
      "Validation Batch number: 348, Validation: Loss: 5.1189, Accuracy: 0.2500\n",
      "Validation Batch number: 349, Validation: Loss: 11.6392, Accuracy: 0.0000\n",
      "Validation Batch number: 350, Validation: Loss: 4.2752, Accuracy: 0.2500\n",
      "Validation Batch number: 351, Validation: Loss: 11.1367, Accuracy: 0.0000\n",
      "Validation Batch number: 352, Validation: Loss: 6.0170, Accuracy: 0.2500\n",
      "Validation Batch number: 353, Validation: Loss: 5.9523, Accuracy: 0.5000\n",
      "Validation Batch number: 354, Validation: Loss: 9.8087, Accuracy: 0.0000\n",
      "Validation Batch number: 355, Validation: Loss: 6.6018, Accuracy: 0.2500\n",
      "Validation Batch number: 356, Validation: Loss: 11.0423, Accuracy: 0.0000\n",
      "Validation Batch number: 357, Validation: Loss: 3.7493, Accuracy: 0.5000\n",
      "Validation Batch number: 358, Validation: Loss: 7.7322, Accuracy: 0.2500\n",
      "Validation Batch number: 359, Validation: Loss: 8.2071, Accuracy: 0.2500\n",
      "Validation Batch number: 360, Validation: Loss: 9.3654, Accuracy: 0.0000\n",
      "Validation Batch number: 361, Validation: Loss: 2.7671, Accuracy: 0.5000\n",
      "Validation Batch number: 362, Validation: Loss: 7.6103, Accuracy: 0.0000\n",
      "Validation Batch number: 363, Validation: Loss: 1.4407, Accuracy: 0.7500\n",
      "Validation Batch number: 364, Validation: Loss: 8.9819, Accuracy: 0.0000\n",
      "Validation Batch number: 365, Validation: Loss: 7.9476, Accuracy: 0.0000\n",
      "Validation Batch number: 366, Validation: Loss: 3.3391, Accuracy: 0.5000\n",
      "Validation Batch number: 367, Validation: Loss: 10.3430, Accuracy: 0.0000\n",
      "Validation Batch number: 368, Validation: Loss: 6.0040, Accuracy: 0.2500\n",
      "Validation Batch number: 369, Validation: Loss: 1.9296, Accuracy: 0.7500\n",
      "Validation Batch number: 370, Validation: Loss: 4.4945, Accuracy: 0.2500\n",
      "Validation Batch number: 371, Validation: Loss: 8.5260, Accuracy: 0.0000\n",
      "Validation Batch number: 372, Validation: Loss: 7.5690, Accuracy: 0.2500\n",
      "Validation Batch number: 373, Validation: Loss: 7.0205, Accuracy: 0.2500\n",
      "Validation Batch number: 374, Validation: Loss: 9.2227, Accuracy: 0.0000\n",
      "Validation Batch number: 375, Validation: Loss: 5.8271, Accuracy: 0.5000\n",
      "Validation Batch number: 376, Validation: Loss: 8.8337, Accuracy: 0.2500\n",
      "Validation Batch number: 377, Validation: Loss: 3.3497, Accuracy: 0.7500\n",
      "Validation Batch number: 378, Validation: Loss: 9.9643, Accuracy: 0.0000\n",
      "Validation Batch number: 379, Validation: Loss: 9.4714, Accuracy: 0.0000\n",
      "Validation Batch number: 380, Validation: Loss: 5.5718, Accuracy: 0.2500\n",
      "Validation Batch number: 381, Validation: Loss: 5.7783, Accuracy: 0.2500\n",
      "Validation Batch number: 382, Validation: Loss: 3.3784, Accuracy: 0.2500\n",
      "Validation Batch number: 383, Validation: Loss: 10.6042, Accuracy: 0.0000\n",
      "Validation Batch number: 384, Validation: Loss: 5.2147, Accuracy: 0.2500\n",
      "Validation Batch number: 385, Validation: Loss: 8.3603, Accuracy: 0.2500\n",
      "Validation Batch number: 386, Validation: Loss: 3.8608, Accuracy: 0.2500\n",
      "Validation Batch number: 387, Validation: Loss: 7.8356, Accuracy: 0.0000\n",
      "Validation Batch number: 388, Validation: Loss: 5.4814, Accuracy: 0.5000\n",
      "Validation Batch number: 389, Validation: Loss: 5.3564, Accuracy: 0.0000\n",
      "Validation Batch number: 390, Validation: Loss: 3.9104, Accuracy: 0.5000\n",
      "Validation Batch number: 391, Validation: Loss: 11.1531, Accuracy: 0.0000\n",
      "Validation Batch number: 392, Validation: Loss: 13.6712, Accuracy: 0.0000\n",
      "Validation Batch number: 393, Validation: Loss: 3.2544, Accuracy: 0.5000\n",
      "Validation Batch number: 394, Validation: Loss: 2.2657, Accuracy: 0.7500\n",
      "Validation Batch number: 395, Validation: Loss: 3.3408, Accuracy: 0.5000\n",
      "Validation Batch number: 396, Validation: Loss: 8.1456, Accuracy: 0.2500\n",
      "Validation Batch number: 397, Validation: Loss: 11.5860, Accuracy: 0.0000\n",
      "Validation Batch number: 398, Validation: Loss: 10.2369, Accuracy: 0.0000\n",
      "Validation Batch number: 399, Validation: Loss: 2.5213, Accuracy: 0.5000\n",
      "Validation Batch number: 400, Validation: Loss: 9.4348, Accuracy: 0.2500\n",
      "Validation Batch number: 401, Validation: Loss: 6.8982, Accuracy: 0.2500\n",
      "Validation Batch number: 402, Validation: Loss: 7.5133, Accuracy: 0.5000\n",
      "Validation Batch number: 403, Validation: Loss: 6.7284, Accuracy: 0.0000\n",
      "Validation Batch number: 404, Validation: Loss: 6.7884, Accuracy: 0.2500\n",
      "Validation Batch number: 405, Validation: Loss: 6.9226, Accuracy: 0.2500\n",
      "Validation Batch number: 406, Validation: Loss: 6.3710, Accuracy: 0.5000\n",
      "Validation Batch number: 407, Validation: Loss: 5.1805, Accuracy: 0.5000\n",
      "Validation Batch number: 408, Validation: Loss: 8.8568, Accuracy: 0.2500\n",
      "Validation Batch number: 409, Validation: Loss: 8.0331, Accuracy: 0.2500\n",
      "Validation Batch number: 410, Validation: Loss: 9.1155, Accuracy: 0.0000\n",
      "Validation Batch number: 411, Validation: Loss: 4.8531, Accuracy: 0.2500\n",
      "Validation Batch number: 412, Validation: Loss: 6.7915, Accuracy: 0.0000\n",
      "Validation Batch number: 413, Validation: Loss: 9.2641, Accuracy: 0.0000\n",
      "Validation Batch number: 414, Validation: Loss: 7.3233, Accuracy: 0.2500\n",
      "Validation Batch number: 415, Validation: Loss: 13.3676, Accuracy: 0.0000\n",
      "Validation Batch number: 416, Validation: Loss: 1.4963, Accuracy: 0.7500\n",
      "Validation Batch number: 417, Validation: Loss: 3.8185, Accuracy: 0.5000\n",
      "Validation Batch number: 418, Validation: Loss: 3.5395, Accuracy: 0.5000\n",
      "Validation Batch number: 419, Validation: Loss: 7.1686, Accuracy: 0.2500\n",
      "Validation Batch number: 420, Validation: Loss: 2.3041, Accuracy: 0.7500\n",
      "Validation Batch number: 421, Validation: Loss: 5.3070, Accuracy: 0.5000\n",
      "Validation Batch number: 422, Validation: Loss: 5.8916, Accuracy: 0.2500\n",
      "Validation Batch number: 423, Validation: Loss: 13.6714, Accuracy: 0.0000\n",
      "Validation Batch number: 424, Validation: Loss: 4.1403, Accuracy: 0.2500\n",
      "Validation Batch number: 425, Validation: Loss: 7.1553, Accuracy: 0.5000\n",
      "Validation Batch number: 426, Validation: Loss: 5.8668, Accuracy: 0.2500\n",
      "Validation Batch number: 427, Validation: Loss: 6.8268, Accuracy: 0.2500\n",
      "Validation Batch number: 428, Validation: Loss: 8.8235, Accuracy: 0.0000\n",
      "Validation Batch number: 429, Validation: Loss: 9.3299, Accuracy: 0.0000\n",
      "Validation Batch number: 430, Validation: Loss: 4.8298, Accuracy: 0.2500\n",
      "Validation Batch number: 431, Validation: Loss: 3.0617, Accuracy: 0.5000\n",
      "Validation Batch number: 432, Validation: Loss: 5.4061, Accuracy: 0.2500\n",
      "Validation Batch number: 433, Validation: Loss: 6.9105, Accuracy: 0.2500\n",
      "Validation Batch number: 434, Validation: Loss: 5.7545, Accuracy: 0.2500\n",
      "Validation Batch number: 435, Validation: Loss: 8.7972, Accuracy: 0.2500\n",
      "Validation Batch number: 436, Validation: Loss: 10.2269, Accuracy: 0.0000\n",
      "Validation Batch number: 437, Validation: Loss: 8.4570, Accuracy: 0.0000\n",
      "Validation Batch number: 438, Validation: Loss: 11.7201, Accuracy: 0.0000\n",
      "Validation Batch number: 439, Validation: Loss: 3.8667, Accuracy: 0.5000\n",
      "Validation Batch number: 440, Validation: Loss: 9.1247, Accuracy: 0.0000\n",
      "Validation Batch number: 441, Validation: Loss: 8.4769, Accuracy: 0.2500\n",
      "Validation Batch number: 442, Validation: Loss: 11.9978, Accuracy: 0.2500\n",
      "Validation Batch number: 443, Validation: Loss: 11.2616, Accuracy: 0.0000\n",
      "Validation Batch number: 444, Validation: Loss: 10.2953, Accuracy: 0.0000\n",
      "Validation Batch number: 445, Validation: Loss: 10.0475, Accuracy: 0.0000\n",
      "Validation Batch number: 446, Validation: Loss: 3.8008, Accuracy: 0.5000\n",
      "Validation Batch number: 447, Validation: Loss: 6.5522, Accuracy: 0.5000\n",
      "Validation Batch number: 448, Validation: Loss: 5.1504, Accuracy: 0.5000\n",
      "Validation Batch number: 449, Validation: Loss: 8.4464, Accuracy: 0.2500\n",
      "Validation Batch number: 450, Validation: Loss: 5.4990, Accuracy: 0.2500\n",
      "Validation Batch number: 451, Validation: Loss: 9.7013, Accuracy: 0.5000\n",
      "Validation Batch number: 452, Validation: Loss: 6.2570, Accuracy: 0.2500\n",
      "Validation Batch number: 453, Validation: Loss: 6.2580, Accuracy: 0.2500\n",
      "Validation Batch number: 454, Validation: Loss: 2.6572, Accuracy: 0.7500\n",
      "Validation Batch number: 455, Validation: Loss: 6.0365, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 456, Validation: Loss: 9.2897, Accuracy: 0.2500\n",
      "Validation Batch number: 457, Validation: Loss: 6.2823, Accuracy: 0.2500\n",
      "Validation Batch number: 458, Validation: Loss: 3.7618, Accuracy: 0.7500\n",
      "Validation Batch number: 459, Validation: Loss: 3.7859, Accuracy: 0.5000\n",
      "Validation Batch number: 460, Validation: Loss: 13.0173, Accuracy: 0.2500\n",
      "Validation Batch number: 461, Validation: Loss: 7.9736, Accuracy: 0.0000\n",
      "Validation Batch number: 462, Validation: Loss: 10.6020, Accuracy: 0.2500\n",
      "Validation Batch number: 463, Validation: Loss: 9.3337, Accuracy: 0.0000\n",
      "Validation Batch number: 464, Validation: Loss: 7.3738, Accuracy: 0.2500\n",
      "Validation Batch number: 465, Validation: Loss: 10.4850, Accuracy: 0.0000\n",
      "Validation Batch number: 466, Validation: Loss: 2.6214, Accuracy: 0.7500\n",
      "Validation Batch number: 467, Validation: Loss: 9.7433, Accuracy: 0.0000\n",
      "Validation Batch number: 468, Validation: Loss: 4.0017, Accuracy: 0.5000\n",
      "Validation Batch number: 469, Validation: Loss: 8.3110, Accuracy: 0.0000\n",
      "Validation Batch number: 470, Validation: Loss: 7.0678, Accuracy: 0.2500\n",
      "Validation Batch number: 471, Validation: Loss: 7.1903, Accuracy: 0.2500\n",
      "Validation Batch number: 472, Validation: Loss: 6.5407, Accuracy: 0.2500\n",
      "Validation Batch number: 473, Validation: Loss: 6.3167, Accuracy: 0.2500\n",
      "Validation Batch number: 474, Validation: Loss: 6.4272, Accuracy: 0.2500\n",
      "Validation Batch number: 475, Validation: Loss: 4.9075, Accuracy: 0.5000\n",
      "Validation Batch number: 476, Validation: Loss: 10.4910, Accuracy: 0.0000\n",
      "Validation Batch number: 477, Validation: Loss: 4.8687, Accuracy: 0.5000\n",
      "Validation Batch number: 478, Validation: Loss: 9.3211, Accuracy: 0.2500\n",
      "Validation Batch number: 479, Validation: Loss: 7.1293, Accuracy: 0.2500\n",
      "Validation Batch number: 480, Validation: Loss: 4.3863, Accuracy: 0.2500\n",
      "Validation Batch number: 481, Validation: Loss: 4.3743, Accuracy: 0.5000\n",
      "Validation Batch number: 482, Validation: Loss: 3.3963, Accuracy: 0.7500\n",
      "Validation Batch number: 483, Validation: Loss: 7.4231, Accuracy: 0.2500\n",
      "Validation Batch number: 484, Validation: Loss: 5.8254, Accuracy: 0.2500\n",
      "Validation Batch number: 485, Validation: Loss: 10.7453, Accuracy: 0.2500\n",
      "Validation Batch number: 486, Validation: Loss: 6.6416, Accuracy: 0.5000\n",
      "Validation Batch number: 487, Validation: Loss: 3.3240, Accuracy: 0.5000\n",
      "Validation Batch number: 488, Validation: Loss: 10.6997, Accuracy: 0.0000\n",
      "Validation Batch number: 489, Validation: Loss: 8.9146, Accuracy: 0.0000\n",
      "Validation Batch number: 490, Validation: Loss: 6.4699, Accuracy: 0.2500\n",
      "Validation Batch number: 491, Validation: Loss: 4.7766, Accuracy: 0.5000\n",
      "Validation Batch number: 492, Validation: Loss: 7.5480, Accuracy: 0.2500\n",
      "Validation Batch number: 493, Validation: Loss: 7.0568, Accuracy: 0.2500\n",
      "Validation Batch number: 494, Validation: Loss: 8.5836, Accuracy: 0.0000\n",
      "Validation Batch number: 495, Validation: Loss: 9.4518, Accuracy: 0.0000\n",
      "Validation Batch number: 496, Validation: Loss: 8.8232, Accuracy: 0.5000\n",
      "Validation Batch number: 497, Validation: Loss: 7.0517, Accuracy: 0.2500\n",
      "Validation Batch number: 498, Validation: Loss: 11.1827, Accuracy: 0.2500\n",
      "Validation Batch number: 499, Validation: Loss: 10.9815, Accuracy: 0.0000\n",
      "Validation Batch number: 500, Validation: Loss: 3.1504, Accuracy: 0.7500\n",
      "Validation Batch number: 501, Validation: Loss: 7.2222, Accuracy: 0.2500\n",
      "Validation Batch number: 502, Validation: Loss: 9.4852, Accuracy: 0.0000\n",
      "Validation Batch number: 503, Validation: Loss: 3.2249, Accuracy: 0.5000\n",
      "Validation Batch number: 504, Validation: Loss: 10.6589, Accuracy: 0.0000\n",
      "Validation Batch number: 505, Validation: Loss: 9.0145, Accuracy: 0.2500\n",
      "Validation Batch number: 506, Validation: Loss: 11.2592, Accuracy: 0.0000\n",
      "Validation Batch number: 507, Validation: Loss: 10.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 508, Validation: Loss: 2.7359, Accuracy: 0.5000\n",
      "Validation Batch number: 509, Validation: Loss: 6.3091, Accuracy: 0.2500\n",
      "Validation Batch number: 510, Validation: Loss: 5.0734, Accuracy: 0.2500\n",
      "Validation Batch number: 511, Validation: Loss: 4.1095, Accuracy: 0.5000\n",
      "Validation Batch number: 512, Validation: Loss: 7.9999, Accuracy: 0.2500\n",
      "Validation Batch number: 513, Validation: Loss: 4.8311, Accuracy: 0.5000\n",
      "Validation Batch number: 514, Validation: Loss: 7.0573, Accuracy: 0.2500\n",
      "Validation Batch number: 515, Validation: Loss: 8.4298, Accuracy: 0.0000\n",
      "Validation Batch number: 516, Validation: Loss: 7.4831, Accuracy: 0.5000\n",
      "Validation Batch number: 517, Validation: Loss: 1.8782, Accuracy: 0.7500\n",
      "Validation Batch number: 518, Validation: Loss: 6.3489, Accuracy: 0.0000\n",
      "Validation Batch number: 519, Validation: Loss: 5.5015, Accuracy: 0.2500\n",
      "Validation Batch number: 520, Validation: Loss: 5.2325, Accuracy: 0.2500\n",
      "Validation Batch number: 521, Validation: Loss: 13.7973, Accuracy: 0.0000\n",
      "Validation Batch number: 522, Validation: Loss: 3.1366, Accuracy: 0.5000\n",
      "Validation Batch number: 523, Validation: Loss: 4.5513, Accuracy: 0.2500\n",
      "Validation Batch number: 524, Validation: Loss: 7.4418, Accuracy: 0.2500\n",
      "Validation Batch number: 525, Validation: Loss: 6.0563, Accuracy: 0.2500\n",
      "Validation Batch number: 526, Validation: Loss: 3.8053, Accuracy: 0.2500\n",
      "Validation Batch number: 527, Validation: Loss: 6.6064, Accuracy: 0.2500\n",
      "Validation Batch number: 528, Validation: Loss: 3.8942, Accuracy: 0.5000\n",
      "Validation Batch number: 529, Validation: Loss: 7.0006, Accuracy: 0.2500\n",
      "Validation Batch number: 530, Validation: Loss: 9.9173, Accuracy: 0.0000\n",
      "Validation Batch number: 531, Validation: Loss: 0.4161, Accuracy: 0.7500\n",
      "Validation Batch number: 532, Validation: Loss: 1.3535, Accuracy: 0.7500\n",
      "Validation Batch number: 533, Validation: Loss: 6.2958, Accuracy: 0.2500\n",
      "Validation Batch number: 534, Validation: Loss: 8.3976, Accuracy: 0.0000\n",
      "Validation Batch number: 535, Validation: Loss: 8.0442, Accuracy: 0.2500\n",
      "Validation Batch number: 536, Validation: Loss: 4.1572, Accuracy: 0.5000\n",
      "Validation Batch number: 537, Validation: Loss: 10.2935, Accuracy: 0.0000\n",
      "Validation Batch number: 538, Validation: Loss: 6.8947, Accuracy: 0.2500\n",
      "Validation Batch number: 539, Validation: Loss: 7.9696, Accuracy: 0.2500\n",
      "Validation Batch number: 540, Validation: Loss: 5.6893, Accuracy: 0.2500\n",
      "Validation Batch number: 541, Validation: Loss: 9.8091, Accuracy: 0.2500\n",
      "Validation Batch number: 542, Validation: Loss: 6.4894, Accuracy: 0.2500\n",
      "Validation Batch number: 543, Validation: Loss: 10.0443, Accuracy: 0.2500\n",
      "Validation Batch number: 544, Validation: Loss: 1.1644, Accuracy: 0.7500\n",
      "Validation Batch number: 545, Validation: Loss: 5.1006, Accuracy: 0.5000\n",
      "Validation Batch number: 546, Validation: Loss: 5.8587, Accuracy: 0.5000\n",
      "Validation Batch number: 547, Validation: Loss: 7.8518, Accuracy: 0.2500\n",
      "Validation Batch number: 548, Validation: Loss: 10.0116, Accuracy: 0.2500\n",
      "Validation Batch number: 549, Validation: Loss: 9.6262, Accuracy: 0.0000\n",
      "Validation Batch number: 550, Validation: Loss: 6.1209, Accuracy: 0.2500\n",
      "Validation Batch number: 551, Validation: Loss: 9.4116, Accuracy: 0.2500\n",
      "Validation Batch number: 552, Validation: Loss: 2.4686, Accuracy: 0.7500\n",
      "Validation Batch number: 553, Validation: Loss: 8.2740, Accuracy: 0.0000\n",
      "Validation Batch number: 554, Validation: Loss: 7.5101, Accuracy: 0.2500\n",
      "Validation Batch number: 555, Validation: Loss: 5.0179, Accuracy: 0.5000\n",
      "Validation Batch number: 556, Validation: Loss: 5.5932, Accuracy: 0.5000\n",
      "Validation Batch number: 557, Validation: Loss: 7.3401, Accuracy: 0.2500\n",
      "Validation Batch number: 558, Validation: Loss: 6.1355, Accuracy: 0.5000\n",
      "Validation Batch number: 559, Validation: Loss: 10.1600, Accuracy: 0.0000\n",
      "Validation Batch number: 560, Validation: Loss: 3.2818, Accuracy: 0.5000\n",
      "Validation Batch number: 561, Validation: Loss: 7.9986, Accuracy: 0.2500\n",
      "Validation Batch number: 562, Validation: Loss: 4.0325, Accuracy: 0.5000\n",
      "Validation Batch number: 563, Validation: Loss: 6.0456, Accuracy: 0.2500\n",
      "Validation Batch number: 564, Validation: Loss: 10.5595, Accuracy: 0.0000\n",
      "Validation Batch number: 565, Validation: Loss: 4.2119, Accuracy: 0.5000\n",
      "Validation Batch number: 566, Validation: Loss: 5.0099, Accuracy: 0.2500\n",
      "Validation Batch number: 567, Validation: Loss: 10.6316, Accuracy: 0.0000\n",
      "Validation Batch number: 568, Validation: Loss: 6.1270, Accuracy: 0.2500\n",
      "Validation Batch number: 569, Validation: Loss: 3.7250, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 570, Validation: Loss: 4.9602, Accuracy: 0.5000\n",
      "Validation Batch number: 571, Validation: Loss: 4.6495, Accuracy: 0.5000\n",
      "Validation Batch number: 572, Validation: Loss: 6.8161, Accuracy: 0.2500\n",
      "Validation Batch number: 573, Validation: Loss: 5.3261, Accuracy: 0.2500\n",
      "Validation Batch number: 574, Validation: Loss: 3.8764, Accuracy: 0.5000\n",
      "Validation Batch number: 575, Validation: Loss: 9.2504, Accuracy: 0.0000\n",
      "Validation Batch number: 576, Validation: Loss: 6.4040, Accuracy: 0.2500\n",
      "Validation Batch number: 577, Validation: Loss: 5.3918, Accuracy: 0.5000\n",
      "Validation Batch number: 578, Validation: Loss: 8.1246, Accuracy: 0.2500\n",
      "Validation Batch number: 579, Validation: Loss: 10.4479, Accuracy: 0.0000\n",
      "Validation Batch number: 580, Validation: Loss: 8.7865, Accuracy: 0.0000\n",
      "Validation Batch number: 581, Validation: Loss: 12.7638, Accuracy: 0.0000\n",
      "Validation Batch number: 582, Validation: Loss: 6.1331, Accuracy: 0.2500\n",
      "Validation Batch number: 583, Validation: Loss: 0.3754, Accuracy: 1.0000\n",
      "Validation Batch number: 584, Validation: Loss: 5.8875, Accuracy: 0.2500\n",
      "Validation Batch number: 585, Validation: Loss: 8.1590, Accuracy: 0.0000\n",
      "Validation Batch number: 586, Validation: Loss: 10.0066, Accuracy: 0.0000\n",
      "Validation Batch number: 587, Validation: Loss: 6.0444, Accuracy: 0.2500\n",
      "Validation Batch number: 588, Validation: Loss: 7.4050, Accuracy: 0.5000\n",
      "Validation Batch number: 589, Validation: Loss: 6.3679, Accuracy: 0.0000\n",
      "Validation Batch number: 590, Validation: Loss: 4.3091, Accuracy: 0.5000\n",
      "Validation Batch number: 591, Validation: Loss: 7.6250, Accuracy: 0.2500\n",
      "Validation Batch number: 592, Validation: Loss: 5.2372, Accuracy: 0.5000\n",
      "Validation Batch number: 593, Validation: Loss: 5.1286, Accuracy: 0.5000\n",
      "Validation Batch number: 594, Validation: Loss: 10.6874, Accuracy: 0.0000\n",
      "Validation Batch number: 595, Validation: Loss: 4.6598, Accuracy: 0.2500\n",
      "Validation Batch number: 596, Validation: Loss: 10.9787, Accuracy: 0.2500\n",
      "Validation Batch number: 597, Validation: Loss: 12.3181, Accuracy: 0.0000\n",
      "Validation Batch number: 598, Validation: Loss: 5.8584, Accuracy: 0.5000\n",
      "Validation Batch number: 599, Validation: Loss: 4.7491, Accuracy: 0.2500\n",
      "Validation Batch number: 600, Validation: Loss: 1.7936, Accuracy: 0.7500\n",
      "Validation Batch number: 601, Validation: Loss: 5.4288, Accuracy: 0.5000\n",
      "Validation Batch number: 602, Validation: Loss: 6.4128, Accuracy: 0.0000\n",
      "Validation Batch number: 603, Validation: Loss: 6.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 604, Validation: Loss: 3.3938, Accuracy: 0.5000\n",
      "Validation Batch number: 605, Validation: Loss: 6.8019, Accuracy: 0.2500\n",
      "Validation Batch number: 606, Validation: Loss: 6.8295, Accuracy: 0.5000\n",
      "Validation Batch number: 607, Validation: Loss: 6.2033, Accuracy: 0.2500\n",
      "Validation Batch number: 608, Validation: Loss: 8.8270, Accuracy: 0.0000\n",
      "Validation Batch number: 609, Validation: Loss: 7.8326, Accuracy: 0.5000\n",
      "Validation Batch number: 610, Validation: Loss: 10.4452, Accuracy: 0.5000\n",
      "Validation Batch number: 611, Validation: Loss: 6.2018, Accuracy: 0.2500\n",
      "Validation Batch number: 612, Validation: Loss: 5.5010, Accuracy: 0.2500\n",
      "Validation Batch number: 613, Validation: Loss: 8.1134, Accuracy: 0.0000\n",
      "Validation Batch number: 614, Validation: Loss: 5.8505, Accuracy: 0.5000\n",
      "Validation Batch number: 615, Validation: Loss: 3.8644, Accuracy: 0.5000\n",
      "Validation Batch number: 616, Validation: Loss: 7.8463, Accuracy: 0.2500\n",
      "Validation Batch number: 617, Validation: Loss: 5.7130, Accuracy: 0.5000\n",
      "Validation Batch number: 618, Validation: Loss: 3.0620, Accuracy: 0.5000\n",
      "Validation Batch number: 619, Validation: Loss: 5.8867, Accuracy: 0.5000\n",
      "Validation Batch number: 620, Validation: Loss: 8.3225, Accuracy: 0.0000\n",
      "Validation Batch number: 621, Validation: Loss: 6.5990, Accuracy: 0.0000\n",
      "Validation Batch number: 622, Validation: Loss: 11.8140, Accuracy: 0.0000\n",
      "Validation Batch number: 623, Validation: Loss: 9.6705, Accuracy: 0.2500\n",
      "Validation Batch number: 624, Validation: Loss: 11.4107, Accuracy: 0.0000\n",
      "Validation Batch number: 625, Validation: Loss: 7.7719, Accuracy: 0.2500\n",
      "Validation Batch number: 626, Validation: Loss: 5.9920, Accuracy: 0.2500\n",
      "Validation Batch number: 627, Validation: Loss: 12.0421, Accuracy: 0.0000\n",
      "Validation Batch number: 628, Validation: Loss: 9.1545, Accuracy: 0.2500\n",
      "Validation Batch number: 629, Validation: Loss: 9.6251, Accuracy: 0.2500\n",
      "Validation Batch number: 630, Validation: Loss: 10.1964, Accuracy: 0.2500\n",
      "Validation Batch number: 631, Validation: Loss: 6.5190, Accuracy: 0.5000\n",
      "Validation Batch number: 632, Validation: Loss: 6.3096, Accuracy: 0.5000\n",
      "Validation Batch number: 633, Validation: Loss: 10.2593, Accuracy: 0.0000\n",
      "Validation Batch number: 634, Validation: Loss: 12.5799, Accuracy: 0.2500\n",
      "Validation Batch number: 635, Validation: Loss: 4.5403, Accuracy: 0.2500\n",
      "Validation Batch number: 636, Validation: Loss: 7.7147, Accuracy: 0.2500\n",
      "Validation Batch number: 637, Validation: Loss: 5.0929, Accuracy: 0.5000\n",
      "Validation Batch number: 638, Validation: Loss: 3.2106, Accuracy: 0.5000\n",
      "Validation Batch number: 639, Validation: Loss: 7.5746, Accuracy: 0.0000\n",
      "Validation Batch number: 640, Validation: Loss: 9.2096, Accuracy: 0.0000\n",
      "Validation Batch number: 641, Validation: Loss: 10.5601, Accuracy: 0.0000\n",
      "Validation Batch number: 642, Validation: Loss: 0.0002, Accuracy: 1.0000\n",
      "Validation Batch number: 643, Validation: Loss: 3.1162, Accuracy: 0.5000\n",
      "Validation Batch number: 644, Validation: Loss: 7.3851, Accuracy: 0.2500\n",
      "Validation Batch number: 645, Validation: Loss: 8.0319, Accuracy: 0.0000\n",
      "Validation Batch number: 646, Validation: Loss: 8.4117, Accuracy: 0.2500\n",
      "Validation Batch number: 647, Validation: Loss: 8.2744, Accuracy: 0.2500\n",
      "Validation Batch number: 648, Validation: Loss: 6.4193, Accuracy: 0.2500\n",
      "Validation Batch number: 649, Validation: Loss: 2.9118, Accuracy: 0.5000\n",
      "Validation Batch number: 650, Validation: Loss: 8.8514, Accuracy: 0.0000\n",
      "Validation Batch number: 651, Validation: Loss: 8.7494, Accuracy: 0.0000\n",
      "Validation Batch number: 652, Validation: Loss: 2.2078, Accuracy: 0.7500\n",
      "Validation Batch number: 653, Validation: Loss: 8.0546, Accuracy: 0.0000\n",
      "Validation Batch number: 654, Validation: Loss: 9.3448, Accuracy: 0.0000\n",
      "Validation Batch number: 655, Validation: Loss: 6.4436, Accuracy: 0.2500\n",
      "Validation Batch number: 656, Validation: Loss: 7.8844, Accuracy: 0.2500\n",
      "Validation Batch number: 657, Validation: Loss: 8.7418, Accuracy: 0.0000\n",
      "Validation Batch number: 658, Validation: Loss: 7.6652, Accuracy: 0.2500\n",
      "Validation Batch number: 659, Validation: Loss: 5.1274, Accuracy: 0.5000\n",
      "Validation Batch number: 660, Validation: Loss: 9.5987, Accuracy: 0.0000\n",
      "Validation Batch number: 661, Validation: Loss: 5.6668, Accuracy: 0.2500\n",
      "Validation Batch number: 662, Validation: Loss: 4.9442, Accuracy: 0.5000\n",
      "Validation Batch number: 663, Validation: Loss: 7.9758, Accuracy: 0.2500\n",
      "Validation Batch number: 664, Validation: Loss: 3.3150, Accuracy: 0.5000\n",
      "Validation Batch number: 665, Validation: Loss: 10.6105, Accuracy: 0.0000\n",
      "Validation Batch number: 666, Validation: Loss: 6.0950, Accuracy: 0.0000\n",
      "Validation Batch number: 667, Validation: Loss: 9.8426, Accuracy: 0.0000\n",
      "Validation Batch number: 668, Validation: Loss: 6.7317, Accuracy: 0.2500\n",
      "Validation Batch number: 669, Validation: Loss: 7.7795, Accuracy: 0.2500\n",
      "Validation Batch number: 670, Validation: Loss: 4.5141, Accuracy: 0.5000\n",
      "Validation Batch number: 671, Validation: Loss: 8.6170, Accuracy: 0.2500\n",
      "Validation Batch number: 672, Validation: Loss: 6.5012, Accuracy: 0.2500\n",
      "Validation Batch number: 673, Validation: Loss: 8.9007, Accuracy: 0.2500\n",
      "Validation Batch number: 674, Validation: Loss: 7.7986, Accuracy: 0.0000\n",
      "Validation Batch number: 675, Validation: Loss: 7.1866, Accuracy: 0.2500\n",
      "Validation Batch number: 676, Validation: Loss: 1.6644, Accuracy: 0.7500\n",
      "Validation Batch number: 677, Validation: Loss: 8.0541, Accuracy: 0.2500\n",
      "Validation Batch number: 678, Validation: Loss: 5.1037, Accuracy: 0.2500\n",
      "Validation Batch number: 679, Validation: Loss: 4.3465, Accuracy: 0.5000\n",
      "Validation Batch number: 680, Validation: Loss: 9.6968, Accuracy: 0.0000\n",
      "Validation Batch number: 681, Validation: Loss: 4.7203, Accuracy: 0.2500\n",
      "Validation Batch number: 682, Validation: Loss: 9.5306, Accuracy: 0.0000\n",
      "Validation Batch number: 683, Validation: Loss: 8.2055, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 684, Validation: Loss: 8.6031, Accuracy: 0.0000\n",
      "Validation Batch number: 685, Validation: Loss: 2.3904, Accuracy: 0.5000\n",
      "Validation Batch number: 686, Validation: Loss: 10.7882, Accuracy: 0.0000\n",
      "Validation Batch number: 687, Validation: Loss: 8.8307, Accuracy: 0.2500\n",
      "Validation Batch number: 688, Validation: Loss: 4.6102, Accuracy: 0.5000\n",
      "Validation Batch number: 689, Validation: Loss: 1.5585, Accuracy: 0.7500\n",
      "Validation Batch number: 690, Validation: Loss: 3.1609, Accuracy: 0.5000\n",
      "Validation Batch number: 691, Validation: Loss: 8.3118, Accuracy: 0.2500\n",
      "Validation Batch number: 692, Validation: Loss: 7.7482, Accuracy: 0.2500\n",
      "Validation Batch number: 693, Validation: Loss: 7.4606, Accuracy: 0.2500\n",
      "Validation Batch number: 694, Validation: Loss: 5.6282, Accuracy: 0.2500\n",
      "Validation Batch number: 695, Validation: Loss: 11.7071, Accuracy: 0.2500\n",
      "Validation Batch number: 696, Validation: Loss: 6.5944, Accuracy: 0.2500\n",
      "Validation Batch number: 697, Validation: Loss: 5.2407, Accuracy: 0.5000\n",
      "Validation Batch number: 698, Validation: Loss: 0.4966, Accuracy: 0.7500\n",
      "Validation Batch number: 699, Validation: Loss: 8.9341, Accuracy: 0.2500\n",
      "Validation Batch number: 700, Validation: Loss: 2.7426, Accuracy: 0.5000\n",
      "Validation Batch number: 701, Validation: Loss: 7.1332, Accuracy: 0.2500\n",
      "Validation Batch number: 702, Validation: Loss: 7.6756, Accuracy: 0.2500\n",
      "Validation Batch number: 703, Validation: Loss: 4.7093, Accuracy: 0.5000\n",
      "Validation Batch number: 704, Validation: Loss: 7.2726, Accuracy: 0.5000\n",
      "Validation Batch number: 705, Validation: Loss: 12.0929, Accuracy: 0.0000\n",
      "Validation Batch number: 706, Validation: Loss: 12.1113, Accuracy: 0.0000\n",
      "Validation Batch number: 707, Validation: Loss: 6.7749, Accuracy: 0.2500\n",
      "Validation Batch number: 708, Validation: Loss: 6.4313, Accuracy: 0.2500\n",
      "Validation Batch number: 709, Validation: Loss: 11.4307, Accuracy: 0.2500\n",
      "Validation Batch number: 710, Validation: Loss: 4.9659, Accuracy: 0.2500\n",
      "Validation Batch number: 711, Validation: Loss: 11.6975, Accuracy: 0.2500\n",
      "Validation Batch number: 712, Validation: Loss: 6.8674, Accuracy: 0.2500\n",
      "Validation Batch number: 713, Validation: Loss: 6.9663, Accuracy: 0.5000\n",
      "Validation Batch number: 714, Validation: Loss: 12.9981, Accuracy: 0.0000\n",
      "Validation Batch number: 715, Validation: Loss: 4.7896, Accuracy: 0.2500\n",
      "Validation Batch number: 716, Validation: Loss: 9.2966, Accuracy: 0.2500\n",
      "Validation Batch number: 717, Validation: Loss: 7.2230, Accuracy: 0.2500\n",
      "Validation Batch number: 718, Validation: Loss: 4.9981, Accuracy: 0.5000\n",
      "Validation Batch number: 719, Validation: Loss: 9.1872, Accuracy: 0.2500\n",
      "Validation Batch number: 720, Validation: Loss: 6.5385, Accuracy: 0.2500\n",
      "Validation Batch number: 721, Validation: Loss: 6.5243, Accuracy: 0.5000\n",
      "Validation Batch number: 722, Validation: Loss: 4.5046, Accuracy: 0.5000\n",
      "Validation Batch number: 723, Validation: Loss: 6.0442, Accuracy: 0.2500\n",
      "Validation Batch number: 724, Validation: Loss: 4.1176, Accuracy: 0.5000\n",
      "Validation Batch number: 725, Validation: Loss: 9.2714, Accuracy: 0.0000\n",
      "Validation Batch number: 726, Validation: Loss: 8.3858, Accuracy: 0.2500\n",
      "Validation Batch number: 727, Validation: Loss: 7.0704, Accuracy: 0.0000\n",
      "Validation Batch number: 728, Validation: Loss: 6.6552, Accuracy: 0.5000\n",
      "Validation Batch number: 729, Validation: Loss: 9.0823, Accuracy: 0.0000\n",
      "Validation Batch number: 730, Validation: Loss: 1.4024, Accuracy: 0.7500\n",
      "Validation Batch number: 731, Validation: Loss: 5.2551, Accuracy: 0.2500\n",
      "Validation Batch number: 732, Validation: Loss: 10.9436, Accuracy: 0.0000\n",
      "Validation Batch number: 733, Validation: Loss: 0.3180, Accuracy: 1.0000\n",
      "Validation Batch number: 734, Validation: Loss: 7.0649, Accuracy: 0.0000\n",
      "Validation Batch number: 735, Validation: Loss: 7.7787, Accuracy: 0.2500\n",
      "Validation Batch number: 736, Validation: Loss: 4.7517, Accuracy: 0.2500\n",
      "Validation Batch number: 737, Validation: Loss: 6.5579, Accuracy: 0.2500\n",
      "Validation Batch number: 738, Validation: Loss: 9.7610, Accuracy: 0.2500\n",
      "Validation Batch number: 739, Validation: Loss: 7.2764, Accuracy: 0.5000\n",
      "Validation Batch number: 740, Validation: Loss: 7.3188, Accuracy: 0.2500\n",
      "Validation Batch number: 741, Validation: Loss: 10.8557, Accuracy: 0.2500\n",
      "Validation Batch number: 742, Validation: Loss: 6.1681, Accuracy: 0.2500\n",
      "Validation Batch number: 743, Validation: Loss: 6.1984, Accuracy: 0.2500\n",
      "Validation Batch number: 744, Validation: Loss: 7.5095, Accuracy: 0.0000\n",
      "Validation Batch number: 745, Validation: Loss: 11.1714, Accuracy: 0.0000\n",
      "Validation Batch number: 746, Validation: Loss: 2.4295, Accuracy: 0.5000\n",
      "Validation Batch number: 747, Validation: Loss: 6.8111, Accuracy: 0.2500\n",
      "Validation Batch number: 748, Validation: Loss: 7.6652, Accuracy: 0.0000\n",
      "Validation Batch number: 749, Validation: Loss: 3.7422, Accuracy: 0.5000\n",
      "Validation Batch number: 750, Validation: Loss: 7.2808, Accuracy: 0.2500\n",
      "Validation Batch number: 751, Validation: Loss: 6.9349, Accuracy: 0.2500\n",
      "Validation Batch number: 752, Validation: Loss: 3.9347, Accuracy: 0.2500\n",
      "Validation Batch number: 753, Validation: Loss: 5.2018, Accuracy: 0.5000\n",
      "Validation Batch number: 754, Validation: Loss: 5.8131, Accuracy: 0.2500\n",
      "Validation Batch number: 755, Validation: Loss: 8.1890, Accuracy: 0.5000\n",
      "Validation Batch number: 756, Validation: Loss: 3.0198, Accuracy: 0.7500\n",
      "Validation Batch number: 757, Validation: Loss: 11.5066, Accuracy: 0.0000\n",
      "Validation Batch number: 758, Validation: Loss: 5.5266, Accuracy: 0.2500\n",
      "Validation Batch number: 759, Validation: Loss: 6.0246, Accuracy: 0.2500\n",
      "Validation Batch number: 760, Validation: Loss: 7.7555, Accuracy: 0.2500\n",
      "Validation Batch number: 761, Validation: Loss: 6.3780, Accuracy: 0.5000\n",
      "Validation Batch number: 762, Validation: Loss: 6.3129, Accuracy: 0.5000\n",
      "Validation Batch number: 763, Validation: Loss: 6.1402, Accuracy: 0.2500\n",
      "Validation Batch number: 764, Validation: Loss: 5.1485, Accuracy: 0.5000\n",
      "Validation Batch number: 765, Validation: Loss: 5.8687, Accuracy: 0.2500\n",
      "Validation Batch number: 766, Validation: Loss: 5.8470, Accuracy: 0.2500\n",
      "Validation Batch number: 767, Validation: Loss: 3.1572, Accuracy: 0.5000\n",
      "Validation Batch number: 768, Validation: Loss: 4.0669, Accuracy: 0.5000\n",
      "Validation Batch number: 769, Validation: Loss: 5.3088, Accuracy: 0.5000\n",
      "Validation Batch number: 770, Validation: Loss: 6.2057, Accuracy: 0.5000\n",
      "Validation Batch number: 771, Validation: Loss: 7.1818, Accuracy: 0.5000\n",
      "Validation Batch number: 772, Validation: Loss: 5.9258, Accuracy: 0.2500\n",
      "Validation Batch number: 773, Validation: Loss: 9.7538, Accuracy: 0.2500\n",
      "Validation Batch number: 774, Validation: Loss: 5.1330, Accuracy: 0.0000\n",
      "Validation Batch number: 775, Validation: Loss: 5.2382, Accuracy: 0.5000\n",
      "Validation Batch number: 776, Validation: Loss: 7.8284, Accuracy: 0.2500\n",
      "Validation Batch number: 777, Validation: Loss: 4.1608, Accuracy: 0.2500\n",
      "Validation Batch number: 778, Validation: Loss: 10.7323, Accuracy: 0.0000\n",
      "Validation Batch number: 779, Validation: Loss: 6.4035, Accuracy: 0.2500\n",
      "Validation Batch number: 780, Validation: Loss: 6.8751, Accuracy: 0.2500\n",
      "Validation Batch number: 781, Validation: Loss: 9.9214, Accuracy: 0.0000\n",
      "Validation Batch number: 782, Validation: Loss: 5.4778, Accuracy: 0.2500\n",
      "Validation Batch number: 783, Validation: Loss: 7.1808, Accuracy: 0.2500\n",
      "Validation Batch number: 784, Validation: Loss: 6.7841, Accuracy: 0.2500\n",
      "Validation Batch number: 785, Validation: Loss: 10.6058, Accuracy: 0.2500\n",
      "Validation Batch number: 786, Validation: Loss: 7.2847, Accuracy: 0.2500\n",
      "Validation Batch number: 787, Validation: Loss: 3.5313, Accuracy: 0.2500\n",
      "Validation Batch number: 788, Validation: Loss: 7.1677, Accuracy: 0.2500\n",
      "Validation Batch number: 789, Validation: Loss: 10.7089, Accuracy: 0.0000\n",
      "Validation Batch number: 790, Validation: Loss: 2.4767, Accuracy: 0.7500\n",
      "Validation Batch number: 791, Validation: Loss: 5.7822, Accuracy: 0.2500\n",
      "Validation Batch number: 792, Validation: Loss: 4.6286, Accuracy: 0.5000\n",
      "Validation Batch number: 793, Validation: Loss: 7.0924, Accuracy: 0.2500\n",
      "Validation Batch number: 794, Validation: Loss: 4.8331, Accuracy: 0.5000\n",
      "Validation Batch number: 795, Validation: Loss: 8.0144, Accuracy: 0.2500\n",
      "Validation Batch number: 796, Validation: Loss: 5.8909, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 797, Validation: Loss: 7.3384, Accuracy: 0.0000\n",
      "Validation Batch number: 798, Validation: Loss: 4.1370, Accuracy: 0.5000\n",
      "Validation Batch number: 799, Validation: Loss: 9.9181, Accuracy: 0.0000\n",
      "Validation Batch number: 800, Validation: Loss: 5.0075, Accuracy: 0.2500\n",
      "Validation Batch number: 801, Validation: Loss: 9.3523, Accuracy: 0.0000\n",
      "Validation Batch number: 802, Validation: Loss: 8.1124, Accuracy: 0.2500\n",
      "Validation Batch number: 803, Validation: Loss: 6.8350, Accuracy: 0.2500\n",
      "Validation Batch number: 804, Validation: Loss: 7.7781, Accuracy: 0.2500\n",
      "Validation Batch number: 805, Validation: Loss: 4.4601, Accuracy: 0.5000\n",
      "Validation Batch number: 806, Validation: Loss: 1.8320, Accuracy: 0.5000\n",
      "Validation Batch number: 807, Validation: Loss: 5.6697, Accuracy: 0.5000\n",
      "Validation Batch number: 808, Validation: Loss: 5.8075, Accuracy: 0.5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m      9\u001b[0m     images,labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m],data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/deeplake/integrations/pytorch/dataset.py:159\u001b[0m, in \u001b[0;36mTorchDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m stream \u001b[38;5;241m=\u001b[39m streaming\u001b[38;5;241m.\u001b[39mread(schedule)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/deeplake/integrations/pytorch/dataset.py:84\u001b[0m, in \u001b[0;36m_process\u001b[0;34m(sample, transform, return_index)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TransformFailedError(index) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/deeplake/integrations/pytorch/common.py:55\u001b[0m, in \u001b[0;36mPytorchTransformFunction.__call__\u001b[0;34m(self, data_in)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tensor, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     54\u001b[0m     value \u001b[38;5;241m=\u001b[39m data_in[tensor]\n\u001b[0;32m---> 55\u001b[0m     data_out[tensor] \u001b[38;5;241m=\u001b[39m value \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m data_out \u001b[38;5;241m=\u001b[39m IterableOrderedDict(data_out)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_out\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torchvision/transforms/transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/torchvision/transforms/functional.py:337\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpimg\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/PIL/Image.py:3103\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3101\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/PIL/Image.py:3027\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3024\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 3027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/PIL/Image.py:2969\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2966\u001b[0m     args \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2968\u001b[0m im \u001b[38;5;241m=\u001b[39m new(mode, size)\n\u001b[0;32m-> 2969\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/anaconda3/envs/lrnconcept/lib/python3.9/site-packages/PIL/Image.py:826\u001b[0m, in \u001b[0;36mImage.frombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    824\u001b[0m d \u001b[38;5;241m=\u001b[39m _getdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, decoder_name, args)\n\u001b[1;32m    825\u001b[0m d\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[0;32m--> 826\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    829\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot enough image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_loss = 0.0\n",
    "valid_acc = 0.0\n",
    "# Validation - No gradient tracking needed\n",
    "with torch.no_grad():\n",
    "    # Set to evaluation mode\n",
    "    net.eval()\n",
    "    # Validation loop\n",
    "    for j, data in enumerate(test_loader):\n",
    "        images,labels = data['images'],data['labels']\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = net(images)\n",
    "        # Compute loss\n",
    "        loss = loss_criterion(outputs,labels.reshape(-1))\n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        valid_loss += loss.item() * images.size(0)\n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        valid_acc += acc.item() * images.size(0)\n",
    "        print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "# Find average training loss and training accuracy\n",
    "avg_train_loss = train_loss/train_data_size \n",
    "avg_train_acc = train_acc/float(train_data_size)\n",
    "# Find average training loss and training accuracy\n",
    "avg_valid_loss = valid_loss/valid_data_size \n",
    "avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "epoch_end = time.time()\n",
    "print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dd86da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f430bbbbc10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.title(\"Train-Validation Accuracy\")\n",
    "\n",
    "plt.plot(train_acc, label='train')\n",
    "\n",
    "plt.plot(valid_acc, label='validation')\n",
    "\n",
    "plt.xlabel('num_epochs', fontsize=12)\n",
    "\n",
    "plt.ylabel('accuracy', fontsize=12)\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e8791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95839acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29c122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841e16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd010b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047ce472",
   "metadata": {},
   "source": [
    "path of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033962ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in datasets/traffic/archive/Test/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/traffic/archive/Test/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_transforms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\datasets\\folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    304\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m ):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 145\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\datasets\\folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\datasets\\folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in datasets/traffic/archive/Test/."
     ]
    }
   ],
   "source": [
    "train_data_path = 'datasets/traffic/archive/Test/'\n",
    "train_data = torchvision.datasets.ImageFolder(root = train_data_path, transform = data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa403355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "690e0d15",
   "metadata": {},
   "source": [
    "data loader for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "valloader = data.DataLoader(val_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lrnconcept] *",
   "language": "python",
   "name": "conda-env-lrnconcept-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
